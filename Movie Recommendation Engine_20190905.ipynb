{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data & Content Analytics Project\n",
    "\n",
    "**Students FullName:** Spanos Nikolaos, Baratsas Sotirios <br/>\n",
    "**Students ID:** f2821826, f2821803 <br/>\n",
    "**Supervisor:** Papageorgious Xaris, Perakis Georgios <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left\">  <strong> <font size=\"3\"> Executive Summary </strong> </div>\n",
    "    <div style=\"text-align: justify\"> This jupyter notebook presents the python code and all the steps of execution that has been followed to create a movie recommendation engine. The notebook is splitted into two parts. The first part contains 4 different units, using data found on IMDB. While the second part, is a different approach using wikipedia's dataset. <p> <p>\n",
    "        <i> Part 1 </i><br>\n",
    "    <b> Unit 1: </b> Read, update & clean the data <br/>\n",
    "    <b> Unit 2: </b> Building the movie recommendation model <br/>\n",
    "    <b> Unit 3: </b> Word Embeddings <br/>\n",
    "        <b> Unit 4: </b> Movie Recommendation algorithm </p><br/></div>\n",
    "     <div style=\"text-align: justify\">Each unit should be executed in the order written for the algorithm to work properly. Although, since most of those units may take time to process each different script of code, the reader of this notebook can <b>directly read the pickled (serialised) file right before the beginning of Unit 4</b>.<br>The jupyter notebook is one of the three deliveralbles of this project and its role is to present the code scripts that built the movie recommendation engine. For the reader to understand the logic behind the tools used to build this movie recommendation engine, (s)he is instructed to read the project's report. Last but not least, the algorithm written in Unit 4 is the one that have been passed to the chatbot messenger in order to propose the user each time a different choice of movie.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import all the python modules mendatory for the code to be executed properly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cleaning and preparing the dataset\n",
    "# -> dataframe manipulation\n",
    "# -> text manipulation\n",
    "# -> Web Scrapping\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "\n",
    "# Module to serialize the content produced from the execution of the code\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Module to monitor the progress of a python for loop\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Module to manipulate text in python - NLTK package\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Module to compute word vectorizers and compute the cosine distance\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "\n",
    "# Module to train our own word embdeddings\n",
    "\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the reader of the notebook wants to download any of the above modules locally on his/her laptop, (s)he is advised to use the following *command*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> This is the final dataset that has been created over the units 1 to 3</b>. The reader is advised to go through all the units of this notebook, in order to understand the processed followed to reach to the final data structure.\n",
    "\n",
    "Read the pickled (serialized) dataset with all the transformation and cleaning steps included!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_thousands = pd.read_pickle('C:\\\\Users\\\\dq186sy\\\\Desktop\\\\Big Data Content Analytics\\\\Movie Recommendation System\\\\five_thousands_02092019.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Unit 1: Read, update and clean the data\n",
    "\n",
    "<b>Summary:</b> In the first unit of the notebook, the code for the following three processes is executed:\n",
    "\n",
    "* <u>Reading:</u> The dataset is imported as a pandas dataframe in order to have the values in a tabular format.\n",
    "* <u>Cleaning:</u> The dataset had many noise. For example, many columns had special characters and extra spaces. Moreover, some columns had to be splitted in order for their respective values to be treated separately (i.e genres). Last but not least, some of the columns of the original dataset was irrelevant with the scope of the project, while some attributes were in the wrong data type.\n",
    "* <u>Updating:</u> The dataset was updated, by scraping information from the imbd links provided in column \"movie_imdb_link\". The fields updated were the IMDB Rating and the Cast, while the Plot_Summary of each movie was added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\"><b>Section 1:</b> Read and Clean the dataset</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial raw dataset collected**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"movie_metadata.csv\", encoding = 'UTF-8')\n",
    "dataset = dataset.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Check for duplicate imbd links and remove any duplicates found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = dataset['movie_imdb_link'].value_counts().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_l = []\n",
    "for i in duplicates:\n",
    "    if i > 1:\n",
    "        empty_l.append(i)\n",
    "        \n",
    "len(empty_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it is obvious that the dataset has some links more that one time and this is a problem. So the solution, is to remove those duplicate values and keep the first occurence of each!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_new = dataset.drop_duplicates(subset=['movie_imdb_link'], keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Remove spaces and tabs from the movie title field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5038      Signed Sealed Delivered \n",
       "5039    The Following             \n",
       "5040         A Plague So Pleasant \n",
       "5041             Shanghai Calling \n",
       "5042            My Date with Drew \n",
       "Name: movie_title, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_new.movie_title.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above print the movie \"The Following\" has extra spaces. It is not propered aligned with the other movies. Thus, those extra tabs should be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dq186sy\\Documents\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "dataset_new['movie_title'] = dataset_new['movie_title'].apply(lambda x: re.sub('\\s+', ' ', x).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5038    Signed Sealed Delivered\n",
       "5039              The Following\n",
       "5040       A Plague So Pleasant\n",
       "5041           Shanghai Calling\n",
       "5042          My Date with Drew\n",
       "Name: movie_title, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_new.movie_title.tail(5)\n",
    "\n",
    "# The extra spaces are properly removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Droping the columns that are irrelevant\n",
    "\n",
    "As irrelevant are declared those variables that do not have any relevant contribution to the content of a movie. For example, the facebook likes two movies received does not affect their respective similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4919, 18)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_new = dataset_new.drop(['color', 'num_critic_for_reviews', 'director_facebook_likes', 'actor_3_facebook_likes', \n",
    "                                'actor_1_facebook_likes', 'cast_total_facebook_likes', 'facenumber_in_poster', \n",
    "                                'actor_2_facebook_likes', 'aspect_ratio', 'movie_facebook_likes', 'content_rating'], axis=1)\n",
    "dataset_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Change the order of the remaining 18 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_new = dataset_new[['movie_title', 'imdb_score', 'num_user_for_reviews', 'num_voted_users', 'director_name', \n",
    "                           'actor_1_name', 'actor_2_name', 'actor_3_name', 'plot_keywords', 'gross', 'genres',  'duration', \n",
    "                           'language', 'country', 'budget', 'title_year', 'movie_imdb_link']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5:** Replace \"|\" with \",\" in the \"plot keywords\" and \"genres\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                  avatar|future|marine|native|paraplegic\n",
      "1       goddess|marriage ceremony|marriage proposal|pi...\n",
      "2                     bomb|espionage|sequel|spy|terrorist\n",
      "3       deception|imprisonment|lawlessness|police offi...\n",
      "4                                                     NaN\n",
      "                              ...                        \n",
      "5038               fraud|postal worker|prison|theft|trial\n",
      "5039         cult|fbi|hideout|prison escape|serial killer\n",
      "5040                                                  NaN\n",
      "5041                                                  NaN\n",
      "5042    actress name in title|crush|date|four word tit...\n",
      "Name: plot_keywords, Length: 4919, dtype: object 0       Action|Adventure|Fantasy|Sci-Fi\n",
      "1              Action|Adventure|Fantasy\n",
      "2             Action|Adventure|Thriller\n",
      "3                       Action|Thriller\n",
      "4                           Documentary\n",
      "                     ...               \n",
      "5038                       Comedy|Drama\n",
      "5039       Crime|Drama|Mystery|Thriller\n",
      "5040              Drama|Horror|Thriller\n",
      "5041               Comedy|Drama|Romance\n",
      "5042                        Documentary\n",
      "Name: genres, Length: 4919, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dataset_new.plot_keywords, dataset_new.genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = ['plot_keywords', 'genres']\n",
    "for i in mylist:\n",
    "    dataset_new.loc[:, i] = dataset_new.loc[:, i].str.replace('|', ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6:** Separate the genres of each movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_title</th>\n",
       "      <th>imdb_score</th>\n",
       "      <th>num_user_for_reviews</th>\n",
       "      <th>num_voted_users</th>\n",
       "      <th>director_name</th>\n",
       "      <th>actor_1_name</th>\n",
       "      <th>actor_2_name</th>\n",
       "      <th>actor_3_name</th>\n",
       "      <th>plot_keywords</th>\n",
       "      <th>gross</th>\n",
       "      <th>...</th>\n",
       "      <th>title_year</th>\n",
       "      <th>movie_imdb_link</th>\n",
       "      <th>genre_0</th>\n",
       "      <th>genre_1</th>\n",
       "      <th>genre_2</th>\n",
       "      <th>genre_3</th>\n",
       "      <th>genre_4</th>\n",
       "      <th>genre_5</th>\n",
       "      <th>genre_6</th>\n",
       "      <th>genre_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avatar</td>\n",
       "      <td>7.9</td>\n",
       "      <td>3054.0</td>\n",
       "      <td>886204</td>\n",
       "      <td>James Cameron</td>\n",
       "      <td>CCH Pounder</td>\n",
       "      <td>Joel David Moore</td>\n",
       "      <td>Wes Studi</td>\n",
       "      <td>avatar,future,marine,native,paraplegic</td>\n",
       "      <td>760505847.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>http://www.imdb.com/title/tt0499549/?ref_=fn_t...</td>\n",
       "      <td>Action</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>Fantasy</td>\n",
       "      <td>Sci-Fi</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>7.1</td>\n",
       "      <td>1238.0</td>\n",
       "      <td>471220</td>\n",
       "      <td>Gore Verbinski</td>\n",
       "      <td>Johnny Depp</td>\n",
       "      <td>Orlando Bloom</td>\n",
       "      <td>Jack Davenport</td>\n",
       "      <td>goddess,marriage ceremony,marriage proposal,pi...</td>\n",
       "      <td>309404152.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>http://www.imdb.com/title/tt0449088/?ref_=fn_t...</td>\n",
       "      <td>Action</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>Fantasy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spectre</td>\n",
       "      <td>6.8</td>\n",
       "      <td>994.0</td>\n",
       "      <td>275868</td>\n",
       "      <td>Sam Mendes</td>\n",
       "      <td>Christoph Waltz</td>\n",
       "      <td>Rory Kinnear</td>\n",
       "      <td>Stephanie Sigman</td>\n",
       "      <td>bomb,espionage,sequel,spy,terrorist</td>\n",
       "      <td>200074175.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>http://www.imdb.com/title/tt2379713/?ref_=fn_t...</td>\n",
       "      <td>Action</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Dark Knight Rises</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2701.0</td>\n",
       "      <td>1144337</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>Tom Hardy</td>\n",
       "      <td>Christian Bale</td>\n",
       "      <td>Joseph Gordon-Levitt</td>\n",
       "      <td>deception,imprisonment,lawlessness,police offi...</td>\n",
       "      <td>448130642.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>http://www.imdb.com/title/tt1345836/?ref_=fn_t...</td>\n",
       "      <td>Action</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Star Wars: Episode VII - The Force Awakens</td>\n",
       "      <td>7.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>Doug Walker</td>\n",
       "      <td>Doug Walker</td>\n",
       "      <td>Rob Walker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.imdb.com/title/tt5289954/?ref_=fn_t...</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  movie_title  imdb_score  \\\n",
       "0                                      Avatar         7.9   \n",
       "1    Pirates of the Caribbean: At World's End         7.1   \n",
       "2                                     Spectre         6.8   \n",
       "3                       The Dark Knight Rises         8.5   \n",
       "4  Star Wars: Episode VII - The Force Awakens         7.1   \n",
       "\n",
       "   num_user_for_reviews  num_voted_users      director_name     actor_1_name  \\\n",
       "0                3054.0           886204      James Cameron      CCH Pounder   \n",
       "1                1238.0           471220     Gore Verbinski      Johnny Depp   \n",
       "2                 994.0           275868         Sam Mendes  Christoph Waltz   \n",
       "3                2701.0          1144337  Christopher Nolan        Tom Hardy   \n",
       "4                   NaN                8        Doug Walker      Doug Walker   \n",
       "\n",
       "       actor_2_name          actor_3_name  \\\n",
       "0  Joel David Moore             Wes Studi   \n",
       "1     Orlando Bloom        Jack Davenport   \n",
       "2      Rory Kinnear      Stephanie Sigman   \n",
       "3    Christian Bale  Joseph Gordon-Levitt   \n",
       "4        Rob Walker                   NaN   \n",
       "\n",
       "                                       plot_keywords        gross  ...  \\\n",
       "0             avatar,future,marine,native,paraplegic  760505847.0  ...   \n",
       "1  goddess,marriage ceremony,marriage proposal,pi...  309404152.0  ...   \n",
       "2                bomb,espionage,sequel,spy,terrorist  200074175.0  ...   \n",
       "3  deception,imprisonment,lawlessness,police offi...  448130642.0  ...   \n",
       "4                                                NaN          NaN  ...   \n",
       "\n",
       "  title_year                                    movie_imdb_link      genre_0  \\\n",
       "0     2009.0  http://www.imdb.com/title/tt0499549/?ref_=fn_t...       Action   \n",
       "1     2007.0  http://www.imdb.com/title/tt0449088/?ref_=fn_t...       Action   \n",
       "2     2015.0  http://www.imdb.com/title/tt2379713/?ref_=fn_t...       Action   \n",
       "3     2012.0  http://www.imdb.com/title/tt1345836/?ref_=fn_t...       Action   \n",
       "4        NaN  http://www.imdb.com/title/tt5289954/?ref_=fn_t...  Documentary   \n",
       "\n",
       "     genre_1   genre_2  genre_3 genre_4 genre_5 genre_6 genre_7  \n",
       "0  Adventure   Fantasy   Sci-Fi       0       0       0       0  \n",
       "1  Adventure   Fantasy        0       0       0       0       0  \n",
       "2  Adventure  Thriller        0       0       0       0       0  \n",
       "3   Thriller         0        0       0       0       0       0  \n",
       "4          0         0        0       0       0       0       0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_new = dataset_new.join(dataset_new['genres'].str.split(',', expand=True).add_prefix('genre_').fillna(0))\n",
    "dataset_new.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7:** Clean the numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeircs_list = ['num_user_for_reviews', 'budget', 'title_year']\n",
    "for i in numeircs_list:\n",
    "    dataset_new[i] = dataset_new[i].fillna(0).astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 8:** Remove the rows that refer to TV Series\n",
    "\n",
    "We have noticed that the duration of some rows was less than an hour (60 minutes). This strongly implies that those rows are episodes of a TV serie. Since we would like to recommend only movies, any row that has a duration less than 70 minutes will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_new = dataset_new[dataset_new['duration'] > 70]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point the shape of our dataset is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4779, 25)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point the dataset has 25 columns. However, we won't stop here. We have to enrich the dataset with columns that will add value to the similarity index between different movies. One such feature is the plot summary of each movie.\n",
    "\n",
    "Using the active imdb link we achieved to request the html file of the movie and extract from there the field relevant to the plot summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\"><b>Section 2:</b> Update the dataset using web scraping techniques</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Python Modules used: BeatifulSoup, Requests</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Scrap the Plot Summary\n",
    "\n",
    "Note: The code snipset below is commented, since its completion would demand 1,5 hours. </p>\n",
    "The script was executed once and then was serialised locally for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_thousands = pd.read_pickle('C:\\\\Users\\\\dq186sy\\\\Desktop\\\\Big Data Content Analytics\\\\Movie Recommendation System\\\\five_thousands_02092019.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mylist = []\n",
    "# souplist = []\n",
    "# myfield = []\n",
    "# plot_summary = []\n",
    "# mock_dataset = five_thousands.movie_imdb_link\n",
    "\n",
    "# for i in tqdm(mock_dataset):\n",
    "#     mylist.append(requests.get(i))\n",
    "    \n",
    "# for i in tqdm(mylist):\n",
    "#     souplist.append(BeautifulSoup(i.text))\n",
    "        \n",
    "# for i in tqdm(souplist):\n",
    "#     myfield.append(i.find_all('div', {'class':'plot_summary'}))\n",
    "\n",
    "# for i in tqdm(myfield):\n",
    "#     for x in tqdm(i):\n",
    "#         for y in tqdm(x.find_all('div', {'class':'summary_text'})):\n",
    "#             plot_summary.append(y.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# five_thousands.to_pickle('C:\\\\Users\\\\dq186sy\\\\Desktop\\\\Big Data Content Analytics\\\\Movie Recommendation System\\\\dataset_new_update1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the requests file with the 4779 movies\n",
    "\n",
    "# Save the file\n",
    "\n",
    "# with open('requests.pkl', 'wb') as f:\n",
    "#     pickle.dump(mylist, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Scrap the IMDB Ratings\n",
    "\n",
    "Note: What we noticed on some movies, was the outdated IMDB Rating so we got the latest IMBD Rating based on the online links of each movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import the pickled requests list**, which contains all the html docs downloaded in Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('requests.pkl', 'rb') as f:\n",
    "#     requests_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# souplist = []\n",
    "# myfield = []\n",
    "# ratings = []\n",
    "\n",
    "# for i in tqdm(requests_list):\n",
    "#     souplist.append(BeautifulSoup(i.text))\n",
    "\n",
    "# for i in tqdm(souplist):\n",
    "#     myfield.append(i.find_all('div', {'class':'ratingValue'}))\n",
    "\n",
    "# for i in tqdm(myfield):\n",
    "#     for x in tqdm(i):\n",
    "#         for y in tqdm(x.find_all('span', {'itemprop':'ratingValue'})):\n",
    "#             ratings.append(y.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# five_thousands.to_pickle('C:\\\\Users\\\\dq186sy\\\\Desktop\\\\Big Data Content Analytics\\\\Movie Recommendation System\\\\dataset_new_update2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Step:** Having processed all these steps of cleaning and updating the data, it is time to pickle the dataset up to this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# five_thousands.to_pickle('C:\\\\Users\\\\dq186sy\\\\Desktop\\\\Big Data Content Analytics\\\\Movie Recommendation System\\\\dataset_cleaned_updated_03092019.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 2: Building the movie reccomendation model <p>\n",
    "\n",
    "<div style=\"text-align: justify\"> <b> Summary: </b> In this unit, we will prepare the dataset that has been pickled for the recommendation engine.\n",
    "Some actions needed for the preparation is to select specific features of the dataset, the most relevant ones, and also to combine features together in order for the cosine distance and the word embeddings to work properly.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import the pickled dataset**, generated in Unit 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_thousands = pd.read_pickle('C:\\\\Users\\\\dq186sy\\\\Desktop\\\\Big Data Content Analytics\\\\Movie Recommendation System\\\\dataset_cleaned_updated_03092019.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Selecting the features that will be used to compare the movie together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features= ['director_name', 'actor_1_name', 'actor_2_name', 'actor_3_name', 'plot_keywords', 'genre_0', 'genre_1', 'genre_2',\n",
    "           'genre_3', 'genre_4', 'genre_5', 'genre_6', 'genre_7', 'plot_summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Create the function that will combine those features (specified above) to a unified row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(row):\n",
    "    return row['director_name'] + \" \" + row['actor_1_name'] + \" \" + row['actor_2_name'] + \" \" + row['actor_3_name'] + \" \" + row['plot_keywords'] + \" \" + row['genre_0'] + \" \" + row['genre_1'] + \" \" + row['genre_2'] + \" \" + row['genre_3'] + \" \" + row['genre_4'] + \" \" + row['genre_5'] + \" \" + row['genre_6'] + \" \" + row['genre_7'] + \" \" + row['plot_summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Replace missing values with 'space' and transform each feature to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    five_thousands[feature] = five_thousands[feature].fillna('')\n",
    "    \n",
    "for feature in features:\n",
    "    five_thousands[feature] = five_thousands[feature].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Create the final column of the dataset, which will determine the content of each movie. </p>\n",
    "**Column Name:** \"combined features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_thousands[\"combined_features\"] = five_thousands.apply(combine_features, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle the dataset that has been generated through Unit 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# five_thousands.to_pickle('C:\\\\Users\\\\dq186sy\\\\Desktop\\\\Big Data Content Analytics\\\\Movie Recommendation System\\\\five_thousands_unit_2_03092019.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 3: Word Embeddings <p>\n",
    "\n",
    "<div style=\"text-align: justify\"> <b> Summary:  </b>Using the FastText module I trained the unsupervised dataset to create word embeddings based on the cast, the plot summary, the plot keywords, the director and the genres of the movies. Having trained the dataset then I used the word embeddings to calculate the cosine distance between the movie the user gave as input with the rest of the dataset's movies. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import the pickled dataset** created in the end of Unit 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>movie_title</th>\n",
       "      <th>imdb_score</th>\n",
       "      <th>num_user_for_reviews</th>\n",
       "      <th>num_voted_users</th>\n",
       "      <th>director_name</th>\n",
       "      <th>actor_1_name</th>\n",
       "      <th>actor_2_name</th>\n",
       "      <th>actor_3_name</th>\n",
       "      <th>...</th>\n",
       "      <th>genre_6</th>\n",
       "      <th>genre_7</th>\n",
       "      <th>movie_index</th>\n",
       "      <th>updated_rating</th>\n",
       "      <th>plot_summary</th>\n",
       "      <th>combined_features</th>\n",
       "      <th>combined_actors</th>\n",
       "      <th>average_combined_features</th>\n",
       "      <th>average_cast_vectors</th>\n",
       "      <th>average_plot_vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>7.9</td>\n",
       "      <td>3054</td>\n",
       "      <td>886204</td>\n",
       "      <td>James Cameron</td>\n",
       "      <td>CCH Pounder</td>\n",
       "      <td>Joel David Moore</td>\n",
       "      <td>Wes Studi</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>\\n                    A paraplegic Marine disp...</td>\n",
       "      <td>James Cameron CCH Pounder Joel David Moore Wes...</td>\n",
       "      <td>CCH Pounder,Joel David Moore,Wes Studi</td>\n",
       "      <td>[0.02741548, 0.014202604, 0.09099899, 0.077658...</td>\n",
       "      <td>[-0.030927671, 0.1788817, -0.03483894, 0.00479...</td>\n",
       "      <td>[-0.013443385, 0.052663647, 0.0044201775, 0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>7.1</td>\n",
       "      <td>1238</td>\n",
       "      <td>471220</td>\n",
       "      <td>Gore Verbinski</td>\n",
       "      <td>Johnny Depp</td>\n",
       "      <td>Orlando Bloom</td>\n",
       "      <td>Jack Davenport</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7.1</td>\n",
       "      <td>\\n                    Captain Barbossa, Will T...</td>\n",
       "      <td>Gore Verbinski Johnny Depp Orlando Bloom Jack ...</td>\n",
       "      <td>Johnny Depp,Orlando Bloom,Jack Davenport</td>\n",
       "      <td>[-0.014147306, 0.055645518, 0.01842277, 0.0490...</td>\n",
       "      <td>[-0.054685786, 0.29279318, -0.10224095, 0.0574...</td>\n",
       "      <td>[0.012848631, 0.14669019, 0.024647376, 0.08664...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Spectre</td>\n",
       "      <td>6.8</td>\n",
       "      <td>994</td>\n",
       "      <td>275868</td>\n",
       "      <td>Sam Mendes</td>\n",
       "      <td>Christoph Waltz</td>\n",
       "      <td>Rory Kinnear</td>\n",
       "      <td>Stephanie Sigman</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6.8</td>\n",
       "      <td>\\n                    A cryptic message from 0...</td>\n",
       "      <td>Sam Mendes Christoph Waltz Rory Kinnear Stepha...</td>\n",
       "      <td>Christoph Waltz,Rory Kinnear,Stephanie Sigman</td>\n",
       "      <td>[0.08728486, 0.03535206, 0.053212844, 0.082393...</td>\n",
       "      <td>[-0.1668923, 0.08294236, -0.16238469, 0.078987...</td>\n",
       "      <td>[0.041572966, 0.11594085, 0.02080741, -0.00669...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>The Dark Knight Rises</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2701</td>\n",
       "      <td>1144337</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>Tom Hardy</td>\n",
       "      <td>Christian Bale</td>\n",
       "      <td>Joseph Gordon-Levitt</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8.4</td>\n",
       "      <td>\\n                    Eight years after the Jo...</td>\n",
       "      <td>Christopher Nolan Tom Hardy Christian Bale Jos...</td>\n",
       "      <td>Tom Hardy,Christian Bale,Joseph Gordon-Levitt</td>\n",
       "      <td>[0.024196928, 0.05928087, -0.027536094, 0.0421...</td>\n",
       "      <td>[-0.20388456, 0.20223862, -0.06453689, -0.0548...</td>\n",
       "      <td>[0.017869577, 0.08244453, -0.037199665, -0.025...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>John Carter</td>\n",
       "      <td>6.6</td>\n",
       "      <td>738</td>\n",
       "      <td>212204</td>\n",
       "      <td>Andrew Stanton</td>\n",
       "      <td>Daryl Sabara</td>\n",
       "      <td>Samantha Morton</td>\n",
       "      <td>Polly Walker</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.6</td>\n",
       "      <td>\\n                    Transported to Barsoom, ...</td>\n",
       "      <td>Andrew Stanton Daryl Sabara Samantha Morton Po...</td>\n",
       "      <td>Daryl Sabara,Samantha Morton,Polly Walker</td>\n",
       "      <td>[0.0008429145, 0.019675335, 0.071446545, 0.031...</td>\n",
       "      <td>[-0.093794234, 0.28681445, -0.062326398, -0.02...</td>\n",
       "      <td>[0.034331556, 0.12629311, 0.0005355305, -0.016...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  index                               movie_title  imdb_score  \\\n",
       "0        0      0                                    Avatar         7.9   \n",
       "1        1      1  Pirates of the Caribbean: At World's End         7.1   \n",
       "2        2      2                                   Spectre         6.8   \n",
       "3        3      3                     The Dark Knight Rises         8.5   \n",
       "4        5      5                               John Carter         6.6   \n",
       "\n",
       "   num_user_for_reviews  num_voted_users      director_name     actor_1_name  \\\n",
       "0                  3054           886204      James Cameron      CCH Pounder   \n",
       "1                  1238           471220     Gore Verbinski      Johnny Depp   \n",
       "2                   994           275868         Sam Mendes  Christoph Waltz   \n",
       "3                  2701          1144337  Christopher Nolan        Tom Hardy   \n",
       "4                   738           212204     Andrew Stanton     Daryl Sabara   \n",
       "\n",
       "       actor_2_name          actor_3_name  ... genre_6  genre_7 movie_index  \\\n",
       "0  Joel David Moore             Wes Studi  ...       0        0           1   \n",
       "1     Orlando Bloom        Jack Davenport  ...       0        0           2   \n",
       "2      Rory Kinnear      Stephanie Sigman  ...       0        0           3   \n",
       "3    Christian Bale  Joseph Gordon-Levitt  ...       0        0           4   \n",
       "4   Samantha Morton          Polly Walker  ...       0        0           6   \n",
       "\n",
       "   updated_rating                                       plot_summary  \\\n",
       "0             7.8  \\n                    A paraplegic Marine disp...   \n",
       "1             7.1  \\n                    Captain Barbossa, Will T...   \n",
       "2             6.8  \\n                    A cryptic message from 0...   \n",
       "3             8.4  \\n                    Eight years after the Jo...   \n",
       "4             6.6  \\n                    Transported to Barsoom, ...   \n",
       "\n",
       "                                   combined_features  \\\n",
       "0  James Cameron CCH Pounder Joel David Moore Wes...   \n",
       "1  Gore Verbinski Johnny Depp Orlando Bloom Jack ...   \n",
       "2  Sam Mendes Christoph Waltz Rory Kinnear Stepha...   \n",
       "3  Christopher Nolan Tom Hardy Christian Bale Jos...   \n",
       "4  Andrew Stanton Daryl Sabara Samantha Morton Po...   \n",
       "\n",
       "                                 combined_actors  \\\n",
       "0         CCH Pounder,Joel David Moore,Wes Studi   \n",
       "1       Johnny Depp,Orlando Bloom,Jack Davenport   \n",
       "2  Christoph Waltz,Rory Kinnear,Stephanie Sigman   \n",
       "3  Tom Hardy,Christian Bale,Joseph Gordon-Levitt   \n",
       "4      Daryl Sabara,Samantha Morton,Polly Walker   \n",
       "\n",
       "                           average_combined_features  \\\n",
       "0  [0.02741548, 0.014202604, 0.09099899, 0.077658...   \n",
       "1  [-0.014147306, 0.055645518, 0.01842277, 0.0490...   \n",
       "2  [0.08728486, 0.03535206, 0.053212844, 0.082393...   \n",
       "3  [0.024196928, 0.05928087, -0.027536094, 0.0421...   \n",
       "4  [0.0008429145, 0.019675335, 0.071446545, 0.031...   \n",
       "\n",
       "                                average_cast_vectors  \\\n",
       "0  [-0.030927671, 0.1788817, -0.03483894, 0.00479...   \n",
       "1  [-0.054685786, 0.29279318, -0.10224095, 0.0574...   \n",
       "2  [-0.1668923, 0.08294236, -0.16238469, 0.078987...   \n",
       "3  [-0.20388456, 0.20223862, -0.06453689, -0.0548...   \n",
       "4  [-0.093794234, 0.28681445, -0.062326398, -0.02...   \n",
       "\n",
       "                                average_plot_vectors  \n",
       "0  [-0.013443385, 0.052663647, 0.0044201775, 0.01...  \n",
       "1  [0.012848631, 0.14669019, 0.024647376, 0.08664...  \n",
       "2  [0.041572966, 0.11594085, 0.02080741, -0.00669...  \n",
       "3  [0.017869577, 0.08244453, -0.037199665, -0.025...  \n",
       "4  [0.034331556, 0.12629311, 0.0005355305, -0.016...  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five_thousands = pd.read_pickle('C:\\\\Users\\\\dq186sy\\\\Desktop\\\\Big Data Content Analytics\\\\Movie Recommendation System\\\\five_thousands_plot_cast_feature_embedded_05092019.pkl')\n",
    "five_thousands.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# five_thousands = pd.read_pickle('C:\\\\Users\\\\dq186sy\\\\Desktop\\\\Big Data Content Analytics\\\\Movie Recommendation System\\\\five_thousands_unit_2_03092019.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1.1: Cast Embeddings\n",
    "\n",
    "# def combine_actors(row):\n",
    "#     return str(row['actor_1_name']) + \",\" + str(row['actor_2_name']) + \",\" + str(row['actor_3_name'])\n",
    "\n",
    "# five_thousands[\"combined_actors\"] = five_thousands.apply(combine_actors, axis=1)\n",
    "# five_thousands = five_thousands.reset_index()\n",
    "\n",
    "# with open('actors_embeddings.txt', 'w', encoding=\"utf-8\") as f:\n",
    "#     for text in five_thousands[\"combined_actors\"].tolist():\n",
    "#         f.write(text + '\\n')\n",
    "\n",
    "# Skipgram model (updated)\n",
    "# model = fasttext.train_unsupervised(\"actors_embeddings.txt\", model='skipgram', lr=0.05, dim=100, ws=3, epoch=500)\n",
    "# model.save_model(\"model_file_cast.bin\")\n",
    "\n",
    "# Stpe 1.2\n",
    "\n",
    "# model = fasttext.load_model('model_file_cast.bin')\n",
    "\n",
    "# average_vector_list_cast = []\n",
    "# for i in tqdm(range(len(five_thousands[\"combined_actors\"]))):\n",
    "#     actors = five_thousands[\"combined_actors\"][i].split(',')\n",
    "#     average = np.mean([model[actor] for actor in actors], axis=0)\n",
    "#     average_vector_list_cast.append(average)\n",
    "\n",
    "# five_thousands['average_cast_vectors'] = average_vector_list_cast\n",
    "\n",
    "# #------------------------------------------------------------------------------------------\n",
    "\n",
    "# Step 2.1: Plot Embeddings\n",
    "\n",
    "# with open('plot_summary_embeddings.txt', 'w', encoding=\"utf-8\") as f:\n",
    "#     for text in five_thousands[\"plot_summary\"].tolist():\n",
    "#         f.write(text + '\\n')\n",
    "\n",
    "# Skipgram model (updated)\n",
    "# model = fasttext.train_unsupervised(\"plot_summary_embeddings.txt\", model='skipgram', lr=0.05, dim=300, ws=6, epoch=500)\n",
    "# model.save_model(\"model_file_plot.bin\")\n",
    "\n",
    "# Step 2.2\n",
    "\n",
    "# model = fasttext.load_model('model_file_plot.bin')\n",
    "\n",
    "# average_vector_list_plot = []\n",
    "# for i in tqdm(range(0, len(five_thousands[\"plot_summary\"]))):\n",
    "#     plot = five_thousands[\"plot_summary\"].str.replace(',', '').str.split(' ')[i]\n",
    "#     average = np.mean([model[word] for word in plot], axis=0)\n",
    "#     average_vector_list_plot.append(average)\n",
    "\n",
    "# five_thousands['average_plot_vectors'] = average_vector_list_plot\n",
    "\n",
    "# # Step 3\n",
    "\n",
    "# my_embeddings_array = np.hstack([five_thousands['average_cast_vectors'].apply(pd.Series).values,\n",
    "# five_thousands['average_plot_vectors'].apply(pd.Series).values])\n",
    "\n",
    "# print(my_embeddings_array.shape)\n",
    "\n",
    "# # Step 4: Pickle the word vectors\n",
    "\n",
    "# with open('my_embeddings_array_02092019.pkl', 'wb') as f:\n",
    "#     pickle.dump(my_embeddings_array, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# five_thousands.to_pickle('C:\\\\Users\\\\dq186sy\\\\Desktop\\\\Big Data Content Analytics\\\\Movie Recommendation System\\\\five_thousands_plot_cast_feature_embedded_05092019.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output:** \"my_embeddings_array, which contains the vectors generated by the cast and the plot embeddings <p>\n",
    "**Size:** 4779x400 <p> **Estimated Time of Completion:** 1 hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #------------------------------------------------------------------------------------------\n",
    "\n",
    "# # Step 5.1: Combined Features Embeddings\n",
    "\n",
    "# five_thousands = five_thousands.reset_index()\n",
    "\n",
    "# with open('combined_features_embeddings.txt', 'w', encoding=\"utf-8\") as f:\n",
    "#     for text in five_thousands[\"combined_features\"].tolist():\n",
    "#         f.write(text + '\\n')\n",
    "\n",
    "# # Skipgram model (updated)\n",
    "# model = fasttext.train_unsupervised(\"combined_features_embeddings.txt\", model='skipgram', lr=0.05, dim=300, ws=6, epoch=500)\n",
    "# model.save_model(\"model_file_combined_features.bin\")\n",
    "\n",
    "# #Step 5.2\n",
    "\n",
    "# average_vector_list_combined_features = []\n",
    "# for i in tqdm(range(0, len(five_thousands[\"combined_features\"]))):\n",
    "#     feature = five_thousands[\"combined_features\"].str.split(' ')[i]\n",
    "#     average = np.mean([model[word] for word in feature], axis=0)\n",
    "#     average_vector_list_combined_features.append(average)\n",
    "\n",
    "# five_thousands['average_combined_features'] = average_vector_list_combined_features\n",
    "\n",
    "# my_embeddings_array_updated = np.hstack([five_thousands['average_combined_features'].apply(pd.Series).values])\n",
    "\n",
    "# print(my_embeddings_array_updated.shape)\n",
    "\n",
    "# # Step 6: Pickle the word vectors\n",
    "\n",
    "# with open('my_embeddings_array_updated_02092019.pkl', 'wb') as f:\n",
    "#     pickle.dump(my_embeddings_array_updated, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output:** \"my_embeddings_array_updated, which contains the vectors generated by combined features embeddings <p>\n",
    "**Size:** 4779x300 <p> **Estimated Time of Completion:** 1 hour </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle the word embeddings calculated on step 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('my_embeddings_array_updated_02092019.pkl', 'wb') as f:\n",
    "#     pickle.dump(my_embeddings_array_updated, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle the the final form of the dataset that contains also the cast, plot and combined features Word Embeddings. </p>\n",
    "\n",
    "<b> This is the final form of the dataset that will be used for the movie recommendation engine! </b> </p>\n",
    "<b> So the reader can directrly have a look on this dataset if (s)he wants to understand the structure of the data!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# five_thousands.to_pickle('C:\\\\Users\\\\dq186sy\\\\Desktop\\\\Big Data Content Analytics\\\\Movie Recommendation System\\\\five_thousands_embedded_02092019.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 4: Movie Recommendation Algorithm <p>\n",
    "\n",
    "<div style=\"text-align: justify\"><b> Summary: </b> Create the algorithm that will process the user's inputs and will recommend three different movies.</div></br>\n",
    "The process flow: <br>\n",
    "\n",
    "<b> Phase 1: Get the user's input and transform it to the appropriate form.</b>\n",
    "* Step 1: User gives a movie genre (i.e action, adventure, thriller, etc).<br>\n",
    "* Step 2: User gives his favorite movie based on the genre (i.e action movie, adventure movie, thriller movie, etc).<br>\n",
    "* Step 3: User gives reasons why (s)he likes the movie. (i.e I like \"Spectre\" because James Bond is my favorite 007 agent<br>\n",
    "\n",
    "<b> Phase 2: Slice the dataset based on user's input</b>\n",
    "* <div style=\"text-align: justify\"> Step 4: Based on the movie genre the user gave, we filter the rows of the dataset that match the user's genre (i.e From 5000 movies we locate only the action movies which are ~1200 movies). So from those 1200 \"action\" movies we take all the other relative columns of the initial dataframe. </div>\n",
    "* Step 5: Check with an IF/ELSE condition whether or not the movie provided by the user is in the movie list of the dataset or not. <br>\n",
    "* Step 6: If the movie is in the movie_list then we followe the process of word embeddings and cosine distance. Otherwise, we do not use the cosine distance approach but rather only the scoring functions that we have created. <br>\n",
    "\n",
    "<b> Phase 3: Recommend to the user the three most similar and highly scored movies </b>\n",
    "* Step 7: Calcutlate the movie scoring and propose the three most highly scored movies. <br> <br>\n",
    "    **Scoring parameter 1:** Primary genre. Award those movies were their first or second genre values are matched to the genre given by the user. <br> <br>\n",
    "    **Scoring parameter 2:** IMDB Rating. The movie promoted first should have a higher imdb rating than the other two movies. Although, since we don't want the result to get biased from a high IMDB rate, we don't give a high weight to the IMDB rating. <br><br>\n",
    "    <div style=\"text-align: justify\"> <b>Scoring parameter 3:</b> Number of words. This is the most highly weighted scoring parameter of the two aforementioned parameters. To calculate the number of words, we use the input given my the user (based on his/her preferences about the movie). Then we use the column \"Combined Features\" and we count how many of the words given by the user are found in the column \"Combined Features\". For your information \"Combined Features\" is a column which contains the values of all the other columns (i.e actors, genres, plot_keywords, plot_summary and director's name) in one common text per movie. So the more the words found the higher the scoring of that movie. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a list of the movies that exist in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_list = five_thousands.movie_title.str.lower().str.replace('-', '').str.replace('The', '').str.replace(':', '').tolist()\n",
    "\n",
    "with open('movie_title_list.pkl', 'wb') as f:\n",
    "    pickle.dump(movies_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Version 6 - The last version</b>\n",
    "\n",
    "<b>Updates:</b> <br>\n",
    "    1) Adding the IF/ELSE option so the user do not get a message error if the movie (s)he give as input does not exist in the dataset. <br>\n",
    "    2) Filtering the word embeddings based on the movie genre given by the user.<br>\n",
    "    3) Award the movies, which their genre belongs to columns: \"genre_0\" and \"genre_1\". So for example a movie that has the following genres: Action, Comedy, Drama and a following one that is: Mystery, Action, Romantic will be awarded higher that those movies that their genres is not first or second.<br>\n",
    "    4) Ensemble the function \"find_correct_movie\", to match the movie given as input to the closest of the ones already existed in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me a movie genre (i.e romance, action, adventure): adventure\n",
      "Give me the title of a movie: spectre\n",
      "Now think of some reasons why you like 'spectre':i love spies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dq186sy\\Documents\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:118: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\dq186sy\\Documents\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:188: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\dq186sy\\Documents\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:191: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\dq186sy\\Documents\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:201: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\dq186sy\\Documents\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:203: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Skyfall', '7.7', 'http://www.imdb.com/title/tt1074638/?ref_=fn_tt_tt_1'], ['Quantum of Solace', '6.6', 'http://www.imdb.com/title/tt0830515/?ref_=fn_tt_tt_1'], ['The Adventures of Elmo in Grouchland', '5.8', 'http://www.imdb.com/title/tt0159421/?ref_=fn_tt_tt_1']]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import pickle\n",
    "\n",
    "# Functions used --------------------------------------------------------------------------------------------------\n",
    "\n",
    "def get_index_from_input_movie(user_input):\n",
    "    return five_thousands[five_thousands.movie_title.str.lower().str.replace('-', '').replace('the', '').replace(':', '') == user_input]['index'].values[0]\n",
    "    \n",
    "def stop_and_stem(uncleaned_list):\n",
    "    ps = PorterStemmer()\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stopped_list = [i for i in uncleaned_list if i not in stop]\n",
    "    stemmed_words = [ps.stem(word) for word in stopped_list]\n",
    "    return stemmed_words\n",
    "\n",
    "def search_words(row, list_of_words):\n",
    "    ps = PorterStemmer()\n",
    "    row = [ps.stem(x) for x in row]\n",
    "    counter = 0\n",
    "    for word in list_of_words:\n",
    "        if word in row:\n",
    "            counter = counter + 1\n",
    "    return counter\n",
    "\n",
    "def find_correct_genre(user_input, genre_list):\n",
    "    scores_sim=[]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    for item in genre_list:\n",
    "        ed = nltk.edit_distance(user_input, item)\n",
    "        scores_sim.append(ed)\n",
    "    correct_genre_index = scores_sim.index(min(scores_sim))\n",
    "    correct_genre = genre_list[correct_genre_index].lower()\n",
    "    return correct_genre\n",
    "\n",
    "def find_correct_movie(user_input, movie_list):\n",
    "    scores_similarity=[]\n",
    "\n",
    "    for item in movie_list:\n",
    "        ed = nltk.edit_distance(user_input, item)\n",
    "        scores_similarity.append(ed)\n",
    "    correct_movie_index = scores_similarity.index(min(scores_similarity))\n",
    "    correct_movie = movie_list[correct_movie_index].lower()\n",
    "    return correct_movie\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Import the dataset\n",
    "\n",
    "# five_thousands = pd.read_pickle('C:\\\\Users\\\\dq186sy\\\\Desktop\\\\Big Data Content Analytics\\\\Movie Recommendation System\\\\five_thousands_embedded_02092019.pkl')\n",
    "\n",
    "five_thousands = pd.read_pickle('C:\\\\Users\\\\dq186sy\\\\Desktop\\\\Big Data Content Analytics\\\\Movie Recommendation System\\\\five_thousands_plot_cast_feature_embedded_05092019.pkl')\n",
    "\n",
    "five_thousands = five_thousands.drop(['level_0', 'index'], axis = 1)\n",
    "\n",
    "five_thousands = five_thousands.reset_index()\n",
    "\n",
    "five_thousands['index'] = np.arange(0, len(five_thousands))\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Create the movie_genre list with the unique types of genre \n",
    "\n",
    "movie_genre_first = five_thousands.genre_0.unique().tolist()\n",
    "movie_genre_second = five_thousands.genre_1.unique().tolist()\n",
    "movie_genre_third = five_thousands.genre_2.unique().tolist()\n",
    "movie_genre_fourth = five_thousands.genre_3.unique().tolist()\n",
    "movie_genre_fifth = five_thousands.genre_4.unique().tolist()\n",
    "movie_genre_sixth = five_thousands.genre_5.unique().tolist()\n",
    "movie_genre_seventh = five_thousands.genre_6.unique().tolist()\n",
    "movie_genre_eight = five_thousands.genre_7.unique().tolist()\n",
    "\n",
    "movie_genre_list = np.asarray(movie_genre_first + movie_genre_second + movie_genre_third + movie_genre_fourth + movie_genre_fifth + movie_genre_sixth + movie_genre_seventh + movie_genre_eight)\n",
    "list(movie_genre_list.flatten())\n",
    "movie_genre_list = list(set(movie_genre_list))\n",
    "\n",
    "movie_genre_list = [x.lower() for x in movie_genre_list]\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Phase 1: Get the user's input and transform it to the appropriate form\n",
    "\n",
    "\n",
    "input_one = input(\"Give me a movie genre (i.e romance, action, adventure): \")\n",
    "input_one = find_correct_genre(input_one.lower(), movie_genre_list)\n",
    "\n",
    "input_movie = input(\"Give me the title of a movie: \").lower().replace('-', '').replace('The', '').replace(':', '')\n",
    "\n",
    "input_two = input(\"Now think of some reasons why you like '{}':\".format(input_movie)).lower().replace(',', '').replace('.', '').split(' ')\n",
    "inputs_list = stop_and_stem(input_two)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Using the genre input given by the user, isolate those movies that match the given genre (i.e Action movies)\n",
    "\n",
    "locked_frame = five_thousands.loc[(five_thousands.genre_0.str.lower() == input_one) | (five_thousands.genre_1.str.lower() == input_one) | (five_thousands.genre_2.str.lower() == input_one) | (five_thousands.genre_3.str.lower() == input_one) | (five_thousands.genre_4.str.lower() == input_one) | (five_thousands.genre_5.str.lower() == input_one) | (five_thousands.genre_6.str.lower() == input_one) | (five_thousands.genre_7.str.lower() == input_one)]\n",
    "\n",
    "indexes_list = locked_frame.index.tolist()\n",
    "\n",
    "locked_frame['index'] = np.arange(0, len(locked_frame))\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Phase 2: Slice the dataset based on the user's input\n",
    "\n",
    "\n",
    "# Check of the movie user gave is in the movie list of the dataset\n",
    "\n",
    "with open('C:\\\\Users\\\\dq186sy\\\\Desktop\\\\Big Data Content Analytics\\\\Movie Recommendation System\\\\movie_title_list.pkl', 'rb') as f:\n",
    "    movies_list = pickle.load(f)\n",
    "\n",
    "if input_movie in movies_list:\n",
    "    \n",
    "    input_movie = find_correct_movie(input_movie_before, movies_list)\n",
    "\n",
    "    # Isolate the movie plot of the movie provided from the user [If the movie is part of the dataset].\n",
    "\n",
    "    movie_plot_new = locked_frame['plot_summary'].loc[(locked_frame.movie_title.str.lower().str.replace('-', '').str.replace('The', '').str.replace(':', '') == input_movie)].apply(lambda x: list(set(re.split(' |,|\\n', x.strip().lower())))).values[0]\n",
    "\n",
    "    cleaned_movie_plot = stop_and_stem(movie_plot_new)\n",
    "\n",
    "    plot_user_input_list = inputs_list + cleaned_movie_plot\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    # Get the index of the movie provied by the user\n",
    "\n",
    "    movie_index = get_index_from_input_movie(input_movie)\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    # Get Features Embeddings based on the movie_index\n",
    "\n",
    "    feature_vector = five_thousands['average_combined_features'][five_thousands['index'] == movie_index]\n",
    "\n",
    "    # Get the Embeddings of the movies matched the user's genre (i.e of all the action movies)\n",
    "\n",
    "    with open('C:\\\\Users\\\\dq186sy\\\\Desktop\\\\Big Data Content Analytics\\\\Movie Recommendation System\\\\my_embeddings_array_updated_02092019.pkl', 'rb') as f:\n",
    "        my_embeddings_array_updated = pickle.load(f)\n",
    "\n",
    "    feature_embeddings_array = my_embeddings_array_updated[indexes_list]\n",
    "    \n",
    "\n",
    "    # -------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    # Concatenate the embeddings of the combined features\n",
    "\n",
    "    selected_movie_vector = np.hstack([feature_vector.apply(pd.Series).values])\n",
    "\n",
    "    # Calculate Cosine Distance\n",
    "\n",
    "    cosine_dist = cosine_distances(feature_embeddings_array, selected_movie_vector.reshape(1,-1))\n",
    "\n",
    "    # Get the similar movies & Slice the dataframe on the top 5 most similar movies to the movie given  by the user\n",
    "\n",
    "    movie_return = np.argsort(cosine_dist, axis=None).tolist()[1:6]\n",
    "\n",
    "    locked_frame_new = locked_frame[locked_frame['index'].isin(movie_return)]\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    # Create two new columns \"Unique Words\" + \"Number of words\"\n",
    "\n",
    "    # Create the new column of \"UNIQUE\" words of the combined features\n",
    "    locked_frame_new['unique_words'] = locked_frame_new.combined_features.apply(lambda x: list(set(re.split(' |,|\\n', x.strip().lower()))))\n",
    "\n",
    "    # Create the column \"Number of words\" for each word contained in the unique words column\n",
    "    locked_frame_new['number_of_words'] = locked_frame_new.unique_words.apply(search_words, args=(plot_user_input_list,))\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    # Phase 3: Recommend to the user the three most similar and highly scored movies \n",
    "    \n",
    "    \n",
    "    # Calculate the movie score\n",
    "\n",
    "    primary_genre = list([(locked_frame.genre_0.str.lower() == input_one)*0.1, (locked_frame.genre_1.str.lower() == input_one)*0.1])\n",
    "\n",
    "    locked_frame_new['movie_score'] = 0.3*locked_frame_new.updated_rating.astype(float) + 0.5*locked_frame_new.number_of_words\n",
    "\n",
    "    locked_frame_new['movie_score'] = locked_frame_new['movie_score'] + primary_genre[0] + primary_genre[1]\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    # Give to the user the proper movie recommendation\n",
    "\n",
    "    top_three_rows = locked_frame_new.nlargest(3, 'movie_score')\n",
    "    \n",
    "    top_three_rows.rename(columns={'movie_title':'Movie Title', 'updated_rating':'IMDB Rate', 'movie_imdb_link':\"Movie's Link\"}, inplace=True)\n",
    "\n",
    "    # Recommend the movie\n",
    "\n",
    "    recommendations_list = top_three_rows.loc[:, ['Movie Title', 'IMDB Rate', \"Movie's Link\"]].values.tolist()\n",
    "    \n",
    "    print(recommendations_list)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    plot_user_input_list = inputs_list\n",
    "    \n",
    "    locked_frame['unique_words'] = locked_frame.combined_features.apply(lambda x: list(set(re.split(' |,|\\n', x.strip().lower()))))\n",
    "\n",
    "    locked_frame['number_of_words'] = locked_frame.unique_words.apply(search_words, args=(plot_user_input_list,))\n",
    "    \n",
    "    \n",
    "    # Fase 3: Recommend to the user the three most similar and highly scored movies\n",
    "    \n",
    "    \n",
    "    primary_genre = list([(locked_frame.genre_0.str.lower() == input_one)*0.1, (locked_frame.genre_1.str.lower() == input_one)*0.1])\n",
    "\n",
    "    locked_frame['movie_score'] = 0.3*locked_frame.updated_rating.astype(float) + 0.5*locked_frame.number_of_words\n",
    "    \n",
    "    locked_frame['movie_score'] = locked_frame['movie_score'] + primary_genre[0] + primary_genre[1]\n",
    "    \n",
    "    \n",
    "    # Give to the user the proper movie recommendation\n",
    "\n",
    "    top_three_rows = locked_frame.nlargest(3, 'movie_score')\n",
    "    \n",
    "    top_three_rows.rename(columns={'movie_title':'Movie Title', 'updated_rating':'IMDB Rate', 'movie_imdb_link':\"Movie's Link\"}, inplace=True)\n",
    "\n",
    "    \n",
    "    # Recommend the movie\n",
    "\n",
    "    recommendations_list = top_three_rows.loc[:, ['Movie Title', 'IMDB Rate', \"Movie's Link\"]].values.tolist()\n",
    "    \n",
    "    print(recommendations_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version 5**\n",
    "\n",
    "<b> Updates: </b> <br>\n",
    "1) Use the word embeddings that belong only to the combined features column. So, we did not use the cast and plot embeddings. Because the results were better than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "# Functions used --------------------------------------------------------------------------------------------------\n",
    "\n",
    "def get_index_from_input_movie(user_input):\n",
    "    return five_thousands[five_thousands.movie_title.str.lower().str.replace('-', '').replace('The', '').replace(':', '') == user_input]['index'].values[0]\n",
    "    \n",
    "def stop_and_stem(uncleaned_list):\n",
    "    ps = PorterStemmer()\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stopped_list = [i for i in uncleaned_list if i not in stop]\n",
    "    stemmed_words = [ps.stem(word) for word in stopped_list]\n",
    "    return stemmed_words\n",
    "\n",
    "def search_words(row, list_of_words):\n",
    "    ps = PorterStemmer()\n",
    "    row = [ps.stem(x) for x in row]\n",
    "    counter = 0\n",
    "    for word in list_of_words:\n",
    "        if word in row:\n",
    "            counter = counter + 1\n",
    "    return counter\n",
    "\n",
    "def find_correct_genre(user_input, genre_list):\n",
    "    scores_sim=[]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    for item in genre_list:\n",
    "        ed = nltk.edit_distance(user_input, item)\n",
    "        scores_sim.append(ed)\n",
    "    correct_genre_index = scores_sim.index(min(scores_sim))\n",
    "    correct_genre = genre_list[correct_genre_index].lower()\n",
    "    print(correct_genre)\n",
    "    return correct_genre\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Import the dataset\n",
    "\n",
    "five_thousands = pd.read_pickle('C:\\\\Users\\\\dq186sy\\\\Desktop\\\\Big Data Content Analytics\\\\Movie Recommendation System\\\\five_thousands_embedded_02092019.pkl')\n",
    "\n",
    "five_thousands = five_thousands.drop(['level_0', 'index'], axis = 1)\n",
    "\n",
    "five_thousands = five_thousands.reset_index()\n",
    "\n",
    "five_thousands['index'] = np.arange(0, len(five_thousands))\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Create the movie_genre list with the unique types of genre \n",
    "\n",
    "movie_genre_first = five_thousands.genre_0.unique().tolist()\n",
    "movie_genre_second = five_thousands.genre_1.unique().tolist()\n",
    "movie_genre_third = five_thousands.genre_2.unique().tolist()\n",
    "movie_genre_fourth = five_thousands.genre_3.unique().tolist()\n",
    "movie_genre_fifth = five_thousands.genre_4.unique().tolist()\n",
    "movie_genre_sixth = five_thousands.genre_5.unique().tolist()\n",
    "movie_genre_seventh = five_thousands.genre_6.unique().tolist()\n",
    "movie_genre_eight = five_thousands.genre_7.unique().tolist()\n",
    "\n",
    "movie_genre_list = np.asarray(movie_genre_first + movie_genre_second + movie_genre_third + movie_genre_fourth + movie_genre_fifth + movie_genre_sixth + movie_genre_seventh + movie_genre_eight)\n",
    "list(movie_genre_list.flatten())\n",
    "movie_genre_list = list(set(movie_genre_list))\n",
    "\n",
    "movie_genre_list = [x.lower() for x in movie_genre_list]\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Get inputs from the user and clean them\n",
    "\n",
    "input_one = input(\"Give me a movie genre (i.e romance, action, adventure): \")\n",
    "input_one = find_correct_genre(input_one.lower(), movie_genre_list)\n",
    "\n",
    "input_movie = input(\"Give me the title of a movie: \").lower().replace('-', '').replace('The', '').replace(':', '')\n",
    "\n",
    "input_two = input(\"Now think of some reasons why you like '{}':\".format(input_movie)).lower().replace(',', '').replace('.', '').split(' ')\n",
    "inputs_list = stop_and_stem(input_two)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Using the genre input given by the user, isolate those movies that match the given genre (i.e Action movies)\n",
    "\n",
    "locked_frame = five_thousands.loc[(five_thousands.genre_0.str.lower() == input_one) | (five_thousands.genre_1.str.lower() == input_one) | (five_thousands.genre_2.str.lower() == input_one) | (five_thousands.genre_3.str.lower() == input_one) | (five_thousands.genre_4.str.lower() == input_one) | (five_thousands.genre_5.str.lower() == input_one) | (five_thousands.genre_6.str.lower() == input_one) | (five_thousands.genre_7.str.lower() == input_one)]\n",
    "\n",
    "indexes_list = locked_frame.index.tolist()\n",
    "\n",
    "locked_frame['index'] = np.arange(0, len(locked_frame))\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Isolate the movie plot of the movie provided from the user [If the movie is part of the dataset].\n",
    "\n",
    "movie_plot_new = locked_frame['plot_summary'].loc[(locked_frame.movie_title.str.lower().str.replace('-', '').str.replace('The', '').str.replace(':', '') == input_movie)].apply(lambda x: list(set(re.split(' |,|\\n', x.strip().lower())))).values[0]\n",
    "\n",
    "cleaned_movie_plot = stop_and_stem(movie_plot_new)\n",
    "\n",
    "plot_user_input_list = inputs_list + cleaned_movie_plot\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Get the index of the movie provied by the user\n",
    "\n",
    "movie_index = get_index_from_input_movie(input_movie)\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Get Features Embeddings based on the movie_index\n",
    "\n",
    "feature_vector = five_thousands['average_combined_features'][five_thousands['index'] == movie_index]\n",
    "\n",
    "\n",
    "# Get the Embeddings of the movies matched the user's genre (i.e of all the action movies)\n",
    "\n",
    "with open('C:\\\\Users\\\\dq186sy\\\\Desktop\\\\Big Data Content Analytics\\\\Movie Recommendation System\\\\my_embeddings_array_updated_02092019.pkl', 'rb') as f:\n",
    "    my_embeddings_array_updated = pickle.load(f)\n",
    "\n",
    "genre_embeddings_array = my_embeddings_array_updated[indexes_list]\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Concatenate the embeddings of the combined features\n",
    "\n",
    "selected_movie_vector = np.hstack([feature_vector.apply(pd.Series).values])\n",
    "\n",
    "# Calculate Cosine Distance\n",
    "\n",
    "cosine_dist = cosine_distances(genre_embeddings_array, selected_movie_vector.reshape(1,-1))\n",
    "\n",
    "# Get the similar movies & Slice the dataframe on the top 5 most similar movies to the movie given  by the user\n",
    "\n",
    "movie_return = np.argsort(cosine_dist, axis=None).tolist()[1:6]\n",
    "\n",
    "locked_frame_new = locked_frame[locked_frame['index'].isin(movie_return)]\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Create two new columns \"Unique Words\" + \"Number of words\"\n",
    "\n",
    "# Create the new column of \"UNIQUE\" words of the combined features\n",
    "locked_frame_new['unique_words'] = locked_frame_new.combined_features.apply(lambda x: list(set(re.split(' |,|\\n', x.strip().lower()))))\n",
    "\n",
    "# Create the column \"Number of words\" for each word contained in the unique words column\n",
    "locked_frame_new['number_of_words'] = locked_frame_new.unique_words.apply(search_words, args=(plot_user_input_list,))\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Calculate the movie score\n",
    "\n",
    "primary_genre = list((locked_frame_new.genre_0.str.lower() == input_one)*0.2)\n",
    "\n",
    "locked_frame_new['movie_score'] = 0.05*locked_frame_new.updated_rating.astype(float) + 0.75*locked_frame_new.number_of_words\n",
    "\n",
    "locked_frame_new['movie_score'] = locked_frame_new['movie_score'] + primary_genre\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Give to the user the proper movie recommendation\n",
    "\n",
    "top_three_rows = locked_frame_new.nlargest(3, 'movie_score')\n",
    "top_three_rows.rename(columns={'movie_title':'Movie Title', 'updated_rating':'IMDB Rate', 'movie_imdb_link':\"Movie's Link\"}, inplace=True)\n",
    "\n",
    "# Recommend the movie\n",
    "\n",
    "recommendations_list = top_three_rows.loc[:, ['Movie Title', 'IMDB Rate', \"Movie's Link\"]].values.tolist()\n",
    "print(recommendations_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version 4**\n",
    "\n",
    "<b> Updates: </b><br>\n",
    "1) Create the word embeddings of the cast, the plot_summary and the combined features using the FastText library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code snipset that wiil provide the response\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_index_from_input_movie(user_input):\n",
    "    return five_thousands[five_thousands.movie_title.str.lower().str.replace('-', '').replace('The', '').replace(':', '') == user_input]['index'].values[0]\n",
    "    \n",
    "def stop_and_stem(uncleaned_list):\n",
    "    ps = PorterStemmer()\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stopped_list = [i for i in uncleaned_list if i not in stop]\n",
    "    stemmed_words = [ps.stem(word) for word in stopped_list]\n",
    "    return stemmed_words\n",
    "\n",
    "def search_words(row, list_of_words):\n",
    "    ps = PorterStemmer()\n",
    "    row = [ps.stem(x) for x in row]\n",
    "    counter = 0\n",
    "    for word in list_of_words:\n",
    "        if word in row:\n",
    "            counter = counter + 1\n",
    "    return counter\n",
    "\n",
    "def find_correct_genre(user_input, genre_list):\n",
    "    scores_sim=[]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    for item in genre_list:\n",
    "        ed = nltk.edit_distance(user_input, item)\n",
    "        scores_sim.append(ed)\n",
    "    correct_genre_index = scores_sim.index(min(scores_sim))\n",
    "    correct_genre = genre_list[correct_genre_index].lower()\n",
    "    print(correct_genre)\n",
    "    return correct_genre\n",
    "\n",
    "# Create the movie_genre list with the unique types of genre \n",
    "\n",
    "five_thousands = pd.read_pickle('C:\\\\Users\\\\dq186sy\\\\Desktop\\\\Big Data Content Analytics\\\\Movie Recommendation System\\\\five_thousands_embedded_02092019.pkl')\n",
    "\n",
    "five_thousands = five_thousands.drop(['level_0', 'index'], axis = 1)\n",
    "\n",
    "five_thousands = five_thousands.reset_index()\n",
    "\n",
    "five_thousands['index'] = np.arange(0, len(five_thousands))\n",
    "\n",
    "movie_genre_first = five_thousands.genre_0.unique().tolist()\n",
    "movie_genre_second = five_thousands.genre_1.unique().tolist()\n",
    "movie_genre_third = five_thousands.genre_2.unique().tolist()\n",
    "movie_genre_fourth = five_thousands.genre_3.unique().tolist()\n",
    "movie_genre_fifth = five_thousands.genre_4.unique().tolist()\n",
    "movie_genre_sixth = five_thousands.genre_5.unique().tolist()\n",
    "movie_genre_seventh = five_thousands.genre_6.unique().tolist()\n",
    "movie_genre_eight = five_thousands.genre_7.unique().tolist()\n",
    "\n",
    "movie_genre_list = np.asarray(movie_genre_first + movie_genre_second + movie_genre_third + movie_genre_fourth + movie_genre_fifth + movie_genre_sixth + movie_genre_seventh + movie_genre_eight)\n",
    "list(movie_genre_list.flatten())\n",
    "movie_genre_list = list(set(movie_genre_list))\n",
    "\n",
    "movie_genre_list = [x.lower() for x in movie_genre_list]\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "# Get inputs from the user and clean them\n",
    "\n",
    "input_one = input(\"Give me a movie genre (i.e romance, action, adventure): \")\n",
    "input_one = find_correct_genre(input_one.lower(), movie_genre_list)\n",
    "\n",
    "input_movie = input(\"Give me the title of a movie: \").lower().replace('-', '').replace('The', '').replace(':', '')\n",
    "\n",
    "input_two = input(\"Now think of some reasons why you like '{}':\".format(input_movie)).lower().replace(',', '').replace('.', '').split(' ')\n",
    "inputs_list = stop_and_stem(input_two)\n",
    "\n",
    "# Calculate the score functions (\"IMDB score\" and \"Included number of words\")\n",
    "\n",
    "# Process User's Input \n",
    "\n",
    "#movie_plot = five_thousands['plot_summary'].loc[(five_thousands.movie_title.str.lower() == input_movie)]\n",
    "\n",
    "movie_plot_new = five_thousands['plot_summary'].loc[(five_thousands.movie_title.str.lower().str.replace('-', '').str.replace('The', '').str.replace(':', '') == input_movie)].apply(lambda x: list(set(re.split(' |,|\\n', x.strip().lower())))).values[0]\n",
    "cleaned_movie_plot = stop_and_stem(movie_plot_new)\n",
    "plot_user_input_list = inputs_list + cleaned_movie_plot\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Similarity\n",
    "\n",
    "# Get movie_index\n",
    "movie_index = get_index_from_input_movie(input_movie)\n",
    "\n",
    "# Calculate distance based on the trained word embeddings\n",
    "\n",
    "# Get Cast Embeddings\n",
    "\n",
    "cast_vector = five_thousands['average_cast_vectors'][five_thousands['index'] == movie_index]\n",
    "\n",
    "# Get Plot Embeddings\n",
    "\n",
    "plot_vector = five_thousands['average_plot_vectors'][five_thousands['index'] == movie_index]\n",
    "\n",
    "# Get Features Embeddings\n",
    "\n",
    "feature_vector = five_thousands['average_combined_features'][five_thousands['index'] == movie_index]\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "# Concatenate the embeddings of cast and plot\n",
    "\n",
    "selected_movie_vector = np.hstack([feature_vector.apply(pd.Series).values])\n",
    "\n",
    "# Calculate Cosine Distance\n",
    "\n",
    "cosine_dist = cosine_distances(my_embeddings_array_updated, selected_movie_vector.reshape(1,-1))\n",
    "\n",
    "# Get the similar movies\n",
    "\n",
    "movie_return = np.argsort(cosine_dist, axis=None).tolist()[1:5]\n",
    "five_thousands_new = five_thousands[five_thousands['index'].isin(movie_return)]\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "# Create two new columns\n",
    "\n",
    "# Create the new column of \"UNIQUE\" words of the combined features\n",
    "five_thousands_new['unique_words'] = five_thousands_new.combined_features.apply(lambda x: list(set(re.split(' |,|\\n', x.strip().lower()))))\n",
    "\n",
    "# Create the column \"Number of words\" for each word contained in the unique words column\n",
    "five_thousands_new['number_of_words'] = five_thousands_new.unique_words.apply(search_words, args=(plot_user_input_list,))\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "# Calculate the movie score\n",
    "primary_genre = list((five_thousands_new.genre_0.str.lower() == input_one)*0.2)\n",
    "\n",
    "five_thousands_new['movie_score'] = 0.05*five_thousands_new.updated_rating.astype(float) + 0.75*five_thousands_new.number_of_words\n",
    "five_thousands_new['movie_score'] = five_thousands_new['movie_score'] + primary_genre\n",
    "\n",
    "# Give to the user the proper movie recommendation\n",
    "\n",
    "top_three_rows = five_thousands_new.nlargest(3, 'movie_score')\n",
    "top_three_rows.rename(columns={'movie_title':'Movie Title', 'updated_rating':'IMDB Rate', 'movie_imdb_link':\"Movie's Link\"}, inplace=True)\n",
    "\n",
    "recommendations_list = top_three_rows.loc[:, ['Movie Title', 'IMDB Rate', \"Movie's Link\"]].values.tolist()\n",
    "print(recommendations_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version 3**\n",
    "\n",
    "<b> Updates: </b><br>\n",
    "1) Use the cosine distance instead of cosine similarity.<br>\n",
    "2) Use the value of the cosine distance as a movie scoring factor. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code snipset that wiil provide the response\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_index_from_input_movie(user_input):\n",
    "    return locked_frame[locked_frame.movie_title.str.lower() == user_input]['index'].values[0]\n",
    "    \n",
    "def stop_and_stem(uncleaned_list):\n",
    "    ps = PorterStemmer()\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stopped_list = [i for i in uncleaned_list if i not in stop]\n",
    "    stemmed_words = [ps.stem(word) for word in stopped_list]\n",
    "    return stemmed_words\n",
    "\n",
    "def search_words(row, list_of_words):\n",
    "    ps = PorterStemmer()\n",
    "    row = [ps.stem(x) for x in row]\n",
    "    counter = 0\n",
    "    for word in list_of_words:\n",
    "        if word in row:\n",
    "            counter = counter + 1\n",
    "    return counter\n",
    "\n",
    "def find_correct_genre(user_input, genre_list):\n",
    "    scores_sim=[]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    for item in genre_list:\n",
    "        ed = nltk.edit_distance(user_input, item)\n",
    "        scores_sim.append(ed)\n",
    "    correct_genre_index = scores_sim.index(min(scores_sim))\n",
    "    correct_genre = genre_list[correct_genre_index].lower()\n",
    "    print(correct_genre)\n",
    "    return correct_genre\n",
    "\n",
    "# Create the movie_genre list with the unique types of genre \n",
    "\n",
    "five_thousands_old = pd.read_pickle('C:\\\\Users\\\\dq186sy\\\\Desktop\\\\Big Data Content Analytics\\\\Movie Recommendation System\\\\five_thousands_model.pkl')\n",
    "\n",
    "five_thousands_old = five_thousands_old.reset_index()\n",
    "\n",
    "five_thousands = five_thousands_old.drop_duplicates(subset=['movie_imdb_link'], keep='first')\n",
    "\n",
    "five_thousands['movie_title'] = five_thousands['movie_title'].apply(lambda x: re.sub('\\s+', ' ', x).strip())\n",
    "\n",
    "movie_genre_first = five_thousands.genre_0.unique().tolist()\n",
    "movie_genre_second = five_thousands.genre_1.unique().tolist()\n",
    "movie_genre_third = five_thousands.genre_2.unique().tolist()\n",
    "movie_genre_fourth = five_thousands.genre_3.unique().tolist()\n",
    "movie_genre_fifth = five_thousands.genre_4.unique().tolist()\n",
    "movie_genre_sixth = five_thousands.genre_5.unique().tolist()\n",
    "movie_genre_seventh = five_thousands.genre_6.unique().tolist()\n",
    "movie_genre_eight = five_thousands.genre_7.unique().tolist()\n",
    "\n",
    "movie_genre_list = np.asarray(movie_genre_first + movie_genre_second + movie_genre_third + movie_genre_fourth + movie_genre_fifth + movie_genre_sixth + movie_genre_seventh + movie_genre_eight)\n",
    "list(movie_genre_list.flatten())\n",
    "movie_genre_list = list(set(movie_genre_list))\n",
    "\n",
    "movie_genre_list = [x.lower() for x in movie_genre_list]\n",
    "\n",
    "# Get inputs from the user and clean them\n",
    "\n",
    "input_one = input(\"Give me a movie genre (i.e romance, action, adventure): \")\n",
    "input_one = find_correct_genre(input_one.lower(), movie_genre_list)\n",
    "\n",
    "input_movie = input(\"Give me the title of a(n) {} movie: \".format(input_one)).lower().replace('-', '').replace('The', '').replace(':', '')\n",
    "\n",
    "input_two = input(\"Now think of some reasons why you like '{}':\".format(input_movie)).lower().split(' ')\n",
    "inputs_list = stop_and_stem(input_two)\n",
    "\n",
    "# Calculate the score functions (\"IMDB score\" and \"Included number of words\")\n",
    "\n",
    "# Process User's Input \n",
    "\n",
    "locked_frame = five_thousands.loc[(five_thousands.genre_0.str.lower() == input_one) | (five_thousands.genre_1.str.lower() == input_one) | (five_thousands.genre_2.str.lower() == input_one) | (five_thousands.genre_3.str.lower() == input_one) | (five_thousands.genre_4.str.lower() == input_one) | (five_thousands.genre_5.str.lower() == input_one) | (five_thousands.genre_6.str.lower() == input_one) | (five_thousands.genre_7.str.lower() == input_one)]\n",
    "\n",
    "locked_frame['index'] = np.arange(0, len(locked_frame))\n",
    "\n",
    "movie_plot = locked_frame['plot_summary'].loc[(locked_frame.movie_title.str.lower() == input_movie)]\n",
    "\n",
    "if len(movie_plot) == 0:\n",
    "    plot_user_input_list = inputs_list\n",
    "\n",
    "else:\n",
    "    movie_plot_new = locked_frame['plot_summary'].loc[(locked_frame.movie_title.str.lower().str.replace('-', '').replace('The', '').replace(':', '') == input_movie)].apply(lambda x: list(set(re.split(' |,|\\n', x.strip().lower())))).values[0]\n",
    "    cleaned_movie_plot = stop_and_stem(movie_plot_new)\n",
    "    plot_user_input_list = inputs_list + cleaned_movie_plot\n",
    "\n",
    "# Process the similarity of combined features\n",
    "\n",
    "# Get movie_index\n",
    "movie_index = get_index_from_input_movie(input_movie)\n",
    "\n",
    "# Calculate distance\n",
    "cv = CountVectorizer()\n",
    "count_matrix = cv.fit_transform(locked_frame['combined_features'])\n",
    "cosine_sim = cosine_similarity(count_matrix)\n",
    "cosine_distance = 1 - cosine_sim\n",
    "\n",
    "# Get the similar movies\n",
    "similar_movies = list(enumerate(cosine_distance[movie_index]))\n",
    "\n",
    "similar_movies_list = []\n",
    "sorted_similar_movies = sorted(similar_movies,key=lambda x:x[1],reverse=False)\n",
    "\n",
    "for element in range(0, len(similar_movies)):\n",
    "    if similar_movies[element][1] <= 0.76:\n",
    "        similar_movies_list.append(similar_movies[element])\n",
    "        \n",
    "similar_movies_list = similar_movies_list[1:]\n",
    "\n",
    "movie_return=[]\n",
    "for index in range(0, len(similar_movies_list)):\n",
    "    movie_return.append(similar_movies_list[index][0])\n",
    "\n",
    "similarity_score = []\n",
    "for score in range(0, len(similar_movies_list)):\n",
    "    similarity_score.append(similar_movies_list[score][1])\n",
    "\n",
    "locked_frame_new = locked_frame[locked_frame['index'].isin(movie_return)]\n",
    "locked_frame_new['unique_words'] = locked_frame_new.combined_features.apply(lambda x: list(set(re.split(' |,|\\n', x.strip().lower()))))\n",
    "locked_frame_new.loc[:, ('similarity_score')] = similarity_score\n",
    "\n",
    "locked_frame_new['number_of_words'] = locked_frame_new.unique_words.apply(search_words, args=(plot_user_input_list,))\n",
    "\n",
    "# Calculate the movie score\n",
    "primary_genre = list((locked_frame_new.genre_0.str.lower() == input_one)*0.1)\n",
    "\n",
    "locked_frame_new['movie_score'] = 0.05*locked_frame_new.updated_rating.astype(float) + 0.65*locked_frame_new.number_of_words + (-0.2*locked_frame_new.similarity_score)\n",
    "locked_frame_new['movie_score'] = locked_frame_new['movie_score'] + primary_genre\n",
    "\n",
    "# Give to the user the proper movie recommendation\n",
    "\n",
    "four_rows = locked_frame_new.nlargest(4, 'movie_score')\n",
    "four_rows.rename(columns={'movie_title':'Movie Title', 'updated_rating':'IMDB Rate', 'movie_imdb_link':\"Movie's Link\"}, inplace=True)\n",
    "\n",
    "if len(movie_plot) == 0:\n",
    "    recommendations_list = four_rows.loc[:, ['Movie Title', 'IMDB Rate', \"Movie's Link\"]].values.tolist()[0:3]\n",
    "\n",
    "else:\n",
    "    recommendations_list = four_rows.loc[:, ['Movie Title', 'IMDB Rate', \"Movie's Link\"]].values.tolist()[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version 2**\n",
    "\n",
    "<b> Updates </b><br>\n",
    "1) Create the first IF/ELSE statement to filter if the movie existed in the dataset or not. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code snipset that wiil provide the response\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def stop_and_stem(uncleaned_list):\n",
    "    ps = PorterStemmer()\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stopped_list = [i for i in uncleaned_list if i not in stop]\n",
    "    stemmed_words = [ps.stem(word) for word in stopped_list]\n",
    "    return stemmed_words\n",
    "\n",
    "def search_words(row, list_of_words):\n",
    "    ps = PorterStemmer()\n",
    "    row = [ps.stem(x) for x in row]\n",
    "    counter = 0\n",
    "    for word in list_of_words:\n",
    "        if word in row:\n",
    "            counter = counter + 1\n",
    "    return counter\n",
    "\n",
    "def find_correct_genre(user_input, genre_list):\n",
    "    scores_sim=[]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    for item in genre_list:\n",
    "        ed = nltk.edit_distance(user_input, item)\n",
    "        scores_sim.append(ed)\n",
    "    correct_genre_index = scores_sim.index(min(scores_sim))\n",
    "    correct_genre = genre_list[correct_genre_index].lower()\n",
    "    print(correct_genre)\n",
    "    return correct_genre\n",
    "\n",
    "# Create the movie_genre list with the unique types of genre \n",
    "\n",
    "five_thousands_old = pd.read_pickle('C:\\\\Users\\\\dq186sy\\\\Desktop\\\\Big Data Content Analytics\\\\Movie Recommendation System\\\\five_thousands_model.pkl')\n",
    "\n",
    "five_thousands = five_thousands_old.drop_duplicates(subset=['movie_imdb_link'], keep='first')\n",
    "\n",
    "movie_genre_first = five_thousands.genre_0.unique().tolist()\n",
    "movie_genre_second = five_thousands.genre_1.unique().tolist()\n",
    "movie_genre_third = five_thousands.genre_2.unique().tolist()\n",
    "movie_genre_fourth = five_thousands.genre_3.unique().tolist()\n",
    "movie_genre_fifth = five_thousands.genre_4.unique().tolist()\n",
    "movie_genre_sixth = five_thousands.genre_5.unique().tolist()\n",
    "movie_genre_seventh = five_thousands.genre_6.unique().tolist()\n",
    "movie_genre_eight = five_thousands.genre_7.unique().tolist()\n",
    "\n",
    "movie_genre_list = np.asarray(movie_genre_first + movie_genre_second + movie_genre_third + movie_genre_fourth + movie_genre_fifth + movie_genre_sixth + movie_genre_seventh + movie_genre_eight)\n",
    "list(movie_genre_list.flatten())\n",
    "movie_genre_list = list(set(movie_genre_list))\n",
    "\n",
    "movie_genre_list = [x.lower() for x in movie_genre_list]\n",
    "\n",
    "# Get inputs from the user and clean them\n",
    "\n",
    "input_one = input(\"Give me a movie genre (i.e romance, action, adventure): \")\n",
    "input_one = find_correct_genre(input_one.lower(), movie_genre_list)\n",
    "\n",
    "input_movie = input(\"Give me the title of a(n) {} movie: \".format(input_one)).lower().replace('-', '')\n",
    "\n",
    "input_two = input(\"Now think of some reasons why you like '{}':\".format(input_movie)).lower().split(' ')\n",
    "inputs_list = stop_and_stem(input_two)\n",
    "\n",
    "# Calculate the score functions (\"IMDB score\" and \"Included number of words\")\n",
    "\n",
    "# Process User's Input \n",
    "\n",
    "locked_frame = five_thousands.loc[(five_thousands.genre_0.str.lower() == input_one) | (five_thousands.genre_1.str.lower() == input_one) | (five_thousands.genre_2.str.lower() == input_one) | (five_thousands.genre_3.str.lower() == input_one) | (five_thousands.genre_4.str.lower() == input_one) | (five_thousands.genre_5.str.lower() == input_one) | (five_thousands.genre_6.str.lower() == input_one) | (five_thousands.genre_7.str.lower() == input_one)]\n",
    "movie_plot = locked_frame['plot_summary'].loc[(locked_frame.movie_title.str.lower() == input_movie)]\n",
    "\n",
    "if len(movie_plot) == 0:\n",
    "    plot_user_input_list = inputs_list\n",
    "\n",
    "else:\n",
    "    movie_plot_new = locked_frame['plot_summary'].loc[(locked_frame.movie_title.str.lower().str.replace('-', '') == input_movie)].apply(lambda x: list(set(re.split(' |,|\\n', x.strip().lower())))).values[0]\n",
    "    cleaned_movie_plot = stop_and_stem(movie_plot_new)\n",
    "    plot_user_input_list = inputs_list + cleaned_movie_plot\n",
    "    \n",
    "# movie_plot_new = locked_frame['plot_summary'].loc[(locked_frame.movie_title.str.lower().str.replace('-', '') == input_movie)].apply(lambda x: list(set(re.split(' |,|\\n', x.strip().lower())))).values[0]\n",
    "# cleaned_movie_plot = stop_and_stem(movie_plot_new)\n",
    "# plot_user_input_list = inputs_list + cleaned_movie_plot\n",
    "\n",
    "locked_frame['unique_words'] = locked_frame.combined_features.apply(lambda x: list(set(re.split(' |,|\\n', x.strip().lower()))))\n",
    "\n",
    "# Process the similarity of combined features\n",
    "\n",
    "cv = CountVectorizer()\n",
    "count_matrix = cv.fit_transform(locked_frame['combined_features'])\n",
    "cosine_sim = cosine_similarity(count_matrix)\n",
    "\n",
    "locked_frame['std_similarity'] = [np.std(x) for x in cosine_sim]\n",
    "locked_frame['median_similarity'] = [np.median(x) for x in cosine_sim]\n",
    "\n",
    "locked_frame['number_of_words'] = locked_frame.unique_words.apply(search_words, args=(plot_user_input_list,))\n",
    "\n",
    "# Calculate the movie score\n",
    "primary_genre = list((locked_frame.genre_0.str.lower() == input_one)*0.1)\n",
    "\n",
    "locked_frame['movie_score'] = 0.05*locked_frame.updated_rating.astype(float) + 0.75*locked_frame.number_of_words + 0.05*locked_frame.std_similarity + 0.05*locked_frame.median_similarity\n",
    "locked_frame['movie_score'] = locked_frame['movie_score'] + primary_genre\n",
    "\n",
    "# Give to the user the proper movie recommendation\n",
    "\n",
    "four_rows = locked_frame.nlargest(4, 'movie_score')\n",
    "four_rows.rename(columns={'movie_title':'Movie Title', 'updated_rating':'IMDB Rate', 'movie_imdb_link':\"Movie's Link\"}, inplace=True)\n",
    "\n",
    "if len(movie_plot) == 0:\n",
    "    recommendations_list = four_rows.loc[:, ['Movie Title', 'IMDB Rate', \"Movie's Link\"]].values.tolist()[0:3]\n",
    "\n",
    "else:\n",
    "    recommendations_list = four_rows.loc[:, ['Movie Title', 'IMDB Rate', \"Movie's Link\"]].values.tolist()[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version 1**\n",
    "\n",
    "1) Use the cosine distance to calculate the similarity amongs the movies. <br>\n",
    "2) Use 5 movie scores. Later this number will drop.<br>\n",
    "3) It is mendatory the movie to exist in the dataset otherwise the algorithm's execution will break.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code snipset that wiil provide the response\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def stop_and_stem(uncleaned_list):\n",
    "    ps = PorterStemmer()\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stopped_list = [i for i in uncleaned_list if i not in stop]\n",
    "    stemmed_words = [ps.stem(word) for word in stopped_list]\n",
    "    return stemmed_words\n",
    "\n",
    "def search_words(row, list_of_words):\n",
    "    ps = PorterStemmer()\n",
    "    row = [ps.stem(x) for x in row]\n",
    "    counter = 0\n",
    "    for word in list_of_words:\n",
    "        if word in row:\n",
    "            counter = counter + 1\n",
    "    return counter\n",
    "\n",
    "def find_correct_genre(user_input, genre_list):\n",
    "    scores_sim=[]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    for item in genre_list:\n",
    "        ed = nltk.edit_distance(user_input, item)\n",
    "        scores_sim.append(ed)\n",
    "    correct_genre_index = scores_sim.index(min(scores_sim))\n",
    "    correct_genre = genre_list[correct_genre_index].lower()\n",
    "    print(correct_genre)\n",
    "    return correct_genre\n",
    "\n",
    "# Create the movie_genre list with the unique types of genre \n",
    "\n",
    "movie_genre_first = five_thousands.genre_0.unique().tolist()\n",
    "movie_genre_second = five_thousands.genre_1.unique().tolist()\n",
    "movie_genre_third = five_thousands.genre_2.unique().tolist()\n",
    "movie_genre_fourth = five_thousands.genre_3.unique().tolist()\n",
    "movie_genre_fifth = five_thousands.genre_4.unique().tolist()\n",
    "movie_genre_sixth = five_thousands.genre_5.unique().tolist()\n",
    "movie_genre_seventh = five_thousands.genre_6.unique().tolist()\n",
    "movie_genre_eight = five_thousands.genre_7.unique().tolist()\n",
    "\n",
    "\n",
    "movie_genre_list = np.asarray(movie_genre_first + movie_genre_second + movie_genre_third + movie_genre_fourth + movie_genre_fifth + movie_genre_sixth + movie_genre_seventh + movie_genre_eight)\n",
    "list(movie_genre_list.flatten())\n",
    "movie_genre_list = list(set(movie_genre_list))\n",
    "\n",
    "movie_genre_list = [x.lower() for x in movie_genre_list]\n",
    "\n",
    "# Get inputs from the user and clean them\n",
    "\n",
    "input_one = input(\"Give me a movie genre (i.e romance, action, adventure): \")\n",
    "input_one = find_correct_genre(input_one.lower(), movie_genre_list)\n",
    "\n",
    "input_two = input(\"Think of a(n) {} movie that you like and write a reason that you like this movie: \".format(input_one)).lower().split(' ')\n",
    "inputs_list = stop_and_stem(input_two)\n",
    "\n",
    "# Calculate the score functions (\"IMDB score\" and \"Included number of words\")\n",
    "\n",
    "# Process User's Input \n",
    "\n",
    "locked_frame = five_thousands.loc[(five_thousands.genre_0.str.lower() == input_one) | (five_thousands.genre_1.str.lower() == input_one) | (five_thousands.genre_2.str.lower() == input_one) | (five_thousands.genre_3.str.lower() == input_one) | (five_thousands.genre_4.str.lower() == input_one) | (five_thousands.genre_5.str.lower() == input_one) | (five_thousands.genre_6.str.lower() == input_one) | (five_thousands.genre_7.str.lower() == input_one)]\n",
    "locked_frame['unique_words'] = locked_frame.combined_features.apply(lambda x: list(set(re.split(' |,|\\n', x.strip().lower()))))\n",
    "\n",
    "# Process the similarity of combined features\n",
    "\n",
    "cv = CountVectorizer()\n",
    "count_matrix = cv.fit_transform(locked_frame['combined_features'])\n",
    "cosine_sim = cosine_similarity(count_matrix)\n",
    "\n",
    "locked_frame['std_similarity'] = [np.std(x) for x in cosine_sim]\n",
    "locked_frame['median_similarity'] = [np.median(x) for x in cosine_sim]\n",
    "\n",
    "locked_frame['number_of_words'] = locked_frame.unique_words.apply(search_words, args=(inputs_list,))\n",
    "\n",
    "# Calculate the movie score\n",
    "primary_genre = list((locked_frame.genre_0.str.lower() == input_one)*0.1)\n",
    "\n",
    "locked_frame['movie_score'] = 0.05*locked_frame.updated_rating.astype(float) + 0.75*locked_frame.number_of_words + 0.05*locked_frame.std_similarity + 0.05*locked_frame.median_similarity\n",
    "locked_frame['movie_score'] = locked_frame['movie_score'] + primary_genre\n",
    "\n",
    "# Give to the user the proper movie recommendation\n",
    "\n",
    "three_rows = locked_frame.nlargest(3, 'movie_score')\n",
    "three_rows.rename(columns={'movie_title':'Movie Title', 'updated_rating':'IMDB Rate', 'movie_imdb_link':\"Movie's Link\"}, inplace=True)\n",
    "recommendations_list = three_rows.loc[:, ['Movie Title', 'IMDB Rate', \"Movie's Link\"]].values.tolist()\n",
    "recommendations_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: A different approach on movie recommendation\n",
    "\n",
    "Creating a Movie Recommendation Engine using Wikipedia links and Neural Network Embeddings\n",
    "\n",
    "Inspired by: https://towardsdatascience.com/building-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "keras_home = '/Users/sotirisbaratsas/.keras/datasets/'\n",
    "from keras.utils import get_file\n",
    "from keras.utils.data_utils import get_file\n",
    "import pandas as pd\n",
    "import gc\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import xml.sax\n",
    "import subprocess\n",
    "import re\n",
    "import mwparserfromhell\n",
    "import json\n",
    "from collections import Counter\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Input, Reshape\n",
    "from keras.layers.merge import Dot\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the wikipedia dump file\n",
    "We start by downloading the Wikipedia dump file that suits our problem from https://dumps.wikimedia.org/enwiki/20190820/. The file we want is the one named \"-pages-articles.xml.bz2\". If we wanted to find a specific article, the multistream version (which is indexed) would be better.\n",
    "Since the filesize is quite large (15.3 GB compressed), we can also work with a few of the partitioned files, which are listed just below the main one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"/Users/sotirisbaratsas/.keras/datasets/enwiki-20190820-pages-articles.xml.bz2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parse the XML pages, we will use a tool called \"Simple API for XML\" (SAX) following the documentation found at http://pyxml.sourceforge.net/topics/howto/section-SAX.html.\n",
    "We are interested in 2 tags for each article in the XML: the <title> tag, which includes the title of the wikipedia article and <text> tag that containts the information of the wikipedia article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiXmlHandler(xml.sax.handler.ContentHandler):\n",
    "    def __init__(self):\n",
    "        xml.sax.handler.ContentHandler.__init__(self)\n",
    "        self._buffer = None\n",
    "        self._values = {}\n",
    "        self._movies = []\n",
    "        self._curent_tag = None\n",
    "\n",
    "    def characters(self, content):\n",
    "        if self._curent_tag:\n",
    "            self._buffer.append(content)\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        if name in ('title', 'text'):\n",
    "            self._curent_tag = name\n",
    "            self._buffer = []\n",
    "\n",
    "    def endElement(self, name):\n",
    "        if name == self._curent_tag:\n",
    "            self._values[name] = ' '.join(self._buffer)\n",
    "\n",
    "        if name == 'page':\n",
    "            movie = process_article(**self._values)\n",
    "            if movie:\n",
    "                self._movies.append(movie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, we define a function to help us identify whether a wikipedia article is a movie.\n",
    "* To parse the article we will use a tool called `mwparserfromhell`, specifically designed to parse wikipedia.\n",
    "* We are in luck, because Wikipedia uses specific templates to help its authors and contributors keep an aligned structure across similar content (e.g. all movies). These templates are called `Infobox templates` and the one we want is called `Infobox film`.\n",
    "* We will program our parser to locate this piece of code in an article and process each movie article, saving the title, the movie properties (e.g. poster, director, producers, etc), as well as the links to other wikipedia articles (i.e. internal links) and external URLs (i.e. external links)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_article(title, text):\n",
    "    wikicode = mwparserfromhell.parse(text)\n",
    "    film = next((template for template in wikicode.filter_templates() \n",
    "                 if template.name.strip().lower() == 'infobox film'), None)\n",
    "    if film:\n",
    "        properties = {param.name.strip_code().strip(): param.value.strip_code().strip() \n",
    "                      for param in film.params\n",
    "                      if param.value.strip_code().strip()\n",
    "                     }\n",
    "        links = [x.title.strip_code().strip() for x in wikicode.filter_wikilinks()]\n",
    "        return (title, properties, links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can now run the parser.\n",
    "* We use `bzcat` to iterate through the compressed file (the uncompressed file is HUGE!).\n",
    "* This might take a while, depending on the number of articles we have parsed.\n",
    "* We could try parsing the partitioned files in parallel to make this quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = xml.sax.make_parser()\n",
    "handler = WikiXmlHandler()\n",
    "parser.setContentHandler(handler)\n",
    "for line in subprocess.Popen(['bzcat'], stdin=open(path), stdout=subprocess.PIPE).stdout:\n",
    "    try:\n",
    "        parser.feed(line)\n",
    "    except StopIteration:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/sotirisbaratsas/.keras/datasets/wp_movies.ndjson', 'wt') as fout:\n",
    "    for movie in handler._movies:\n",
    "         fout.write(json.dumps(movie) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our next step is to load the file with the movies (about 3130 movies) and create a separate list with all the movie titles. We will use this to match a user input to an actual title.\n",
    "* In the process of doing that, we observe that a lot of movie titles are followed by a disambiguation note - e.g. Titanic (1997 film) - to help wikipedia users distinguish articles with the same name.\n",
    "* Using a regular expression, we will remove this part of the string from our movie titles list, since a user is unlikely to provide the movie name in this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/sotirisbaratsas/.keras/datasets/wp_movies.ndjson') as fin:\n",
    "    movies = [json.loads(l) for l in fin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_titles = []\n",
    "for i in range(0, len(movies)):\n",
    "    x = re.sub('\\ \\(.*?\\)', '', movies[i][0])\n",
    "    x = re.sub('the ', '', x.lower())\n",
    "    movie_titles.append(x)\n",
    "    \n",
    "len(movie_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_titles[0:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, we want to get an idea about the links that each movie has.\n",
    "* We will create a counter for the links in each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_counts = Counter()\n",
    "for movie in movies:\n",
    "    link_counts.update(movie[2])\n",
    "# Let's take a look at the 20 most common links for all movies.\n",
    "link_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things easier, we will keep only the link pairs that appear more than 3 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_links = [link for link, c in link_counts.items() if c >= 3]\n",
    "link_to_idx = {link: idx for idx, link in enumerate(top_links)}\n",
    "movie_to_idx = {movie[0]: idx for idx, movie in enumerate(movies)}\n",
    "pairs = []\n",
    "for movie in movies:\n",
    "    pairs.extend((link_to_idx[link], movie_to_idx[movie[0]]) for link in movie[2] if link in link_to_idx)\n",
    "pairs_set = set(pairs)\n",
    "len(pairs), len(top_links), len(movie_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, from the 321.483 pairs of links that appear, we will keep only 31.026 for a total of 3130 unique movies, which should be enough to build our embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the most important step, we need to define the way our embeddings will work. The idea is simple: we will feed the model with pairs of movies and links {movie, link} and train the model to define embeddings of size=50 variables. Our hope is that the model will place movies that link to the same pages closer together and that placement will reflect the similarity of the movies. Moreover, the model will be trained to understand if certain links are similar to each other (e.g. \"Category: Action movies\" will be similar to \"Category: Adventure movies\" since a lot of movies will have both links)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will feed the Movie and the Link as the Input Layer. Each movie and each link will be represented by an integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_embedding_model(embedding_size=50):\n",
    "    link = Input(name='link', shape=(1,))\n",
    "    movie = Input(name='movie', shape=(1,))\n",
    "    link_embedding = Embedding(name='link_embedding', \n",
    "                               input_dim=len(top_links), \n",
    "                               output_dim=embedding_size)(link)\n",
    "    movie_embedding = Embedding(name='movie_embedding', \n",
    "                                input_dim=len(movie_to_idx), \n",
    "                                output_dim=embedding_size)(movie)\n",
    "    dot = Dot(name='dot_product', normalize=True, axes=2)([link_embedding, movie_embedding])\n",
    "    merged = Reshape((1,))(dot)\n",
    "    model = Model(inputs=[link, movie], outputs=[merged])\n",
    "    model.compile(optimizer='nadam', loss='mse')\n",
    "    return model\n",
    "\n",
    "model = movie_embedding_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "\n",
    "def batchifier(pairs, positive_samples=50, negative_ratio=10):\n",
    "    batch_size = positive_samples * (1 + negative_ratio)\n",
    "    batch = np.zeros((batch_size, 3))\n",
    "    while True:\n",
    "        for idx, (link_id, movie_id) in enumerate(random.sample(pairs, positive_samples)):\n",
    "            batch[idx, :] = (link_id, movie_id, 1)\n",
    "        idx = positive_samples\n",
    "        while idx < batch_size:\n",
    "            movie_id = random.randrange(len(movie_to_idx))\n",
    "            link_id = random.randrange(len(top_links))\n",
    "            if not (link_id, movie_id) in pairs_set:\n",
    "                batch[idx, :] = (link_id, movie_id, -1)\n",
    "                idx += 1\n",
    "        np.random.shuffle(batch)\n",
    "        yield {'link': batch[:, 0], 'movie': batch[:, 1]}, batch[:, 2]\n",
    "\n",
    "next(batchifier(pairs, positive_samples=3, negative_ratio=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_samples_per_batch = 512\n",
    "\n",
    "model.fit_generator(\n",
    "    batchifier(pairs, positive_samples=positive_samples_per_batch, negative_ratio=10),\n",
    "    epochs=15,\n",
    "    steps_per_epoch=len(pairs) // positive_samples_per_batch,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input()\n",
    "\n",
    "def find_correct_title(user_input):\n",
    "    scores_sim=[]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    for item in movie_titles:\n",
    "        ed = nltk.edit_distance(user_input, item)\n",
    "        scores_sim.append(ed)\n",
    "    correct_title_index = scores_sim.index(min(scores_sim))\n",
    "    correct_title = movies[correct_title_index][0]\n",
    "    print(correct_title)\n",
    "    return correct_title\n",
    "\n",
    "find_correct_title(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input(\"Give me a movie you like: \").lower().replace('the', '')\n",
    "\n",
    "movie = model.get_layer('movie_embedding')\n",
    "movie_weights = movie.get_weights()[0]\n",
    "movie_lengths = np.linalg.norm(movie_weights, axis=1)\n",
    "normalized_movies = (movie_weights.T / movie_lengths).T\n",
    "\n",
    "def find_correct_title(user_input):\n",
    "    scores_sim=[]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    for item in movie_titles:\n",
    "        ed = nltk.edit_distance(user_input, item)\n",
    "        scores_sim.append(ed)\n",
    "    correct_title_index = scores_sim.index(min(scores_sim))\n",
    "    correct_title = movies[correct_title_index][0]\n",
    "    return(correct_title)\n",
    "\n",
    "def similar_movies(movie):\n",
    "    dists = np.dot(normalized_movies, normalized_movies[movie_to_idx[movie]])\n",
    "    closest = np.argsort(dists)[-10:]\n",
    "    for c in reversed(closest):\n",
    "        print(c, movies[c][0], dists[c])\n",
    "\n",
    "correct_title = find_correct_title(user_input)\n",
    "similar_movies(correct_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes / Sub-section\n",
    "\n",
    "This section containes code that has been implemented although not used. Thus, it is commented only for presentation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pretrained Model by FastText\n",
    "\n",
    "# from gensim.models.keyedvectors import KeyedVectors\n",
    "# vec_model = KeyedVectors.load_word2vec_format('C:/Users/dq186sy/Desktop/Big Data Content Analytics/Movie Recommendation System/crawl-300d-2M.vec', binary=False)\n",
    "\n",
    "# # Cast\n",
    "\n",
    "# def combine_actors(row):\n",
    "#     return row['actor_1_name'] + \",\" + row['actor_2_name'] + \",\" + row['actor_3_name']\n",
    "\n",
    "# five_thousands[\"combined_actors\"] = five_thousands.apply(combine_actors, axis=1)\n",
    "\n",
    "# average_vector_list_cast = []\n",
    "# for i in tqdm(range(len(five_thousands[\"combined_actors\"]))):\n",
    "#     actors = five_thousands[\"combined_actors\"].str.split(',')[i]\n",
    "#     average = np.mean([vec_model[actor] for actor in actors], axis=0)\n",
    "#     average_vector_list_cast.append(average)\n",
    "\n",
    "# five_thousands['average_cast_vectors'] = average_vector_list_cast\n",
    "\n",
    "# # Plot\n",
    "\n",
    "# average_vector_list_plot = []\n",
    "# for i in tqdm(range(len(five_thousands[\"plot_summary\"]))):\n",
    "#     plot = five_thousands[\"plot_summary\"].str.split(' ')[i]\n",
    "#     average = np.mean([vec_model[word] for word in plot], axis=0)\n",
    "#     average_vector_list_plot.append(average)\n",
    "\n",
    "# five_thousands['average_plot_vectors'] = average_vector_list_plot\n",
    "\n",
    "# # Combined Features\n",
    "\n",
    "# average_vector_list_combined_features = []\n",
    "# for i in tqdm(range(0, len(five_thousands[\"combined_features\"]))):\n",
    "#     feature = five_thousands[\"combined_features\"].str.split(' ')[i]\n",
    "#     average = np.mean([model[word] for word in feature], axis=0)\n",
    "#     average_vector_list_combined_features.append(average)\n",
    "\n",
    "# five_thousands['average_combined_features'] = average_vector_list_combined_features\n",
    "\n",
    "# # Vect Embeddings\n",
    "\n",
    "# my_embeddings_array = np.hstack([five_thousands['average_plot_vectors'].apply(pd.Series).values,\n",
    "#                                  five_thousands['average_cast_vectors'].apply(pd.Series).values,\n",
    "#                                  five_thousands['average_combined_features'].apply(pd.Series).values])\n",
    "\n",
    "# my_embeddings_array.shape\n",
    "\n",
    "# # Pickle the word vectors\n",
    "\n",
    "# with open('my_embeddings_array.pkl', 'wb') as f:\n",
    "#     pickle.dump(my_embeddings_array, f)\n",
    "\n",
    "# ------------------------------------------------------------------------------- \n",
    "\n",
    "# Clean the categorical variables [From the moment that I read the dataset with encoding UTF-8] this script is redadunt\n",
    "\n",
    "# # Strip '/xa0' from the strings\n",
    "\n",
    "# categorical_features = []\n",
    "# for var in dataset_new.columns:\n",
    "#     if dataset_new[var].dtypes=='O':\n",
    "#         categorical_features.append(var)\n",
    "        \n",
    "# for i in categorical_features:\n",
    "#     dataset_new[i] = dataset_new[i].astype(str).apply(lambda x: x.replace(u'\\xa0', u''))\n",
    "\n",
    "# dataset_new.actor_1_name[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Webhook with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions on how to use webhook can be found in the two python files also attached with this Jupyter Notebook\n",
    "\n",
    "<b>File 1:</b> app_v6.py <br>\n",
    "<b>Fiel 2:</b> movie_recommendation_v5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run app_v6.py (using the cmd terminal on Windows):\n",
    "\n",
    "Step 1: Set the path directory to: Desktop (if you have saved the app_v6.py file in Desktop) <br>\n",
    "Step 2 (Run the command): python app.py or FLASK_APP=hello.py flask run\n",
    "\n",
    "#### Run the https protocole (using the cmd terminal on Windows): \n",
    "\n",
    "Step 1: Set the path directory to: C:\\Users\\dq186sy\\Desktop\\ngrok-stable-windows-amd64 (or the path where the ngrok.exe is saved) <br>\n",
    "Step 2 (Run the command): ngrok http 5000 <br>\n",
    "Step 3: Copy paste the **https** link that ends to .io (this link is updated every time the command is executed) <br>\n",
    "Step 4: Copy paste the link to dialogflow engine under the tab: fulfilment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
